{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59cb386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, pickle\n",
    "sys.path.append('../')\n",
    "import preprocess\n",
    "from model.moe import MOE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf91d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b467033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "# from deepctr_torch.models.deepfm import *\n",
    "from deepctr_torch.models.basemodel import *\n",
    "from deepctr_torch.models.deepfm import *\n",
    "from deepctr_torch.layers import BiInteractionPooling\n",
    "from deepctr_torch.layers import activation_layer\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from utils import uAUC\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "# TASK_UAUC_WEIGHT = {\n",
    "#     'read_comment':0.4,\n",
    "#     'like': 0.3,\n",
    "#     'click_avatar': 0.2,\n",
    "#     'forward':0.1,\n",
    "#     'favorite': 0.1,\n",
    "#     'comment': 0.1,\n",
    "#     'follow':0.1\n",
    "# }\n",
    "\n",
    "TASK_UAUC_WEIGHT = {\n",
    "    'read_comment':4/13,\n",
    "    'like': 3/13,\n",
    "    'click_avatar': 2/13,\n",
    "    'forward':1/13,\n",
    "    'favorite': 1/13,\n",
    "    'comment': 1/13,\n",
    "    'follow':1/13\n",
    "}\n",
    "\n",
    "class GHM_Loss(nn.Module):\n",
    "    def __init__(self, bins, alpha=0.75, reduction='sum'):\n",
    "        super(GHM_Loss, self).__init__()\n",
    "        self._bins = bins\n",
    "        self._alpha = alpha\n",
    "        self._last_bin_count = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def _g2bin(self, g):\n",
    "        return torch.floor(g * (self._bins - 0.0001)).long()\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        g = torch.abs(self._custom_loss_grad(x, target)).detach()\n",
    "\n",
    "        bin_idx = self._g2bin(g)\n",
    "\n",
    "        bin_count = torch.zeros((self._bins))\n",
    "        for i in range(self._bins):\n",
    "            bin_count[i] = (bin_idx == i).sum().item()\n",
    "\n",
    "        N = (x.size(0) * x.size(1))\n",
    "\n",
    "        if self._last_bin_count is None:\n",
    "            self._last_bin_count = bin_count\n",
    "        else:\n",
    "            bin_count = self._alpha * self._last_bin_count + (1 - self._alpha) * bin_count\n",
    "            self._last_bin_count = bin_count\n",
    "\n",
    "        nonempty_bins = (bin_count > 0).sum().item()\n",
    "\n",
    "        gd = bin_count * nonempty_bins\n",
    "        gd = torch.clamp(gd, min=0.0001)\n",
    "        beta = N / gd\n",
    "\n",
    "        return self._custom_loss(x, target, beta[bin_idx])\n",
    "\n",
    "\n",
    "class GHMC_Loss(GHM_Loss):\n",
    "    # 分类损失\n",
    "    def __init__(self, bins, alpha):\n",
    "        super(GHMC_Loss, self).__init__(bins, alpha)\n",
    "\n",
    "    def _custom_loss(self, x, target, weight=None):\n",
    "        return F.binary_cross_entropy(x, target, weight=weight, reduction=self.reduction)\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        return x.detach() - target\n",
    "\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        # nn.init.kaiming_uniform_(tensor.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **inputs_dim**: input feature dimension.\n",
    "\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024, device='cpu'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                # nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "                # nn.init.kaiming_uniform_(tensor, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.kaiming_normal_(tensor, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.linears[i](deep_input)\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "            fc = self.dropout(fc)\n",
    "            \n",
    "            deep_input = fc\n",
    "        return deep_input\n",
    "\n",
    "\n",
    "\n",
    "def create_embedding_from_pretrained(feature_columns, init_std=0.0001, linear=False, sparse=False, \n",
    "                                     device='cpu', pretrained_embedding_dict=None, frozen_pretrained=False):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # for feat in varlen_sparse_feature_columns:\n",
    "    #     embedding_dict[feat.embedding_name] = nn.EmbeddingBag(\n",
    "    #         feat.dimension, embedding_size, sparse=sparse, mode=feat.combiner)\n",
    "\n",
    "    for name, tensor in embedding_dict.items():\n",
    "        if (pretrained_embedding_dict is not None) and  (name in pretrained_embedding_dict):\n",
    "            tensor.weight.data.copy_(torch.from_numpy(pretrained_embedding_dict[name]))\n",
    "            if frozen_pretrained:\n",
    "                tensor.weight.requires_grad = False\n",
    "        else:\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "class MyBaseModel(BaseModel):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n",
    "                 init_std=0.0001, seed=1024, task='binary', device='cpu', gpus=None, \n",
    "                 pretrained_embedding_dict=None, frozen_pretrained=False, num_tasks=4):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        \n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        self.device = device\n",
    "        self.gpus = gpus\n",
    "        if gpus and str(self.gpus[0]) not in self.device:\n",
    "            raise ValueError(\n",
    "                \"`gpus[0]` should be the same gpu with `device`\")\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n",
    "        # self.embedding_dict = create_embedding_from_pretrained(dnn_feature_columns, init_std, sparse=False, \n",
    "        #                                                       device=device, pretrained_embedding_dict=pretrained_embedding_dict,\n",
    "        #                                                       frozen_pretrained=frozen_pretrained)\n",
    "\n",
    "        self.regularization_weight = []\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "        # parameters of callbacks\n",
    "        self._is_graph_network = True  # used for ModelCheckpoint\n",
    "        self.stop_training = False  # used for EarlyStopping\n",
    "        self.history = History()\n",
    "\n",
    "    def fit(self, train_loader, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, val_userid_list=None, shuffle=True, callbacks=None, num_warm_epochs=1, \n",
    "            lr_scheduler=True, scheduler_epochs=5, reduction='sum', task_weight=None, task_dict=None,\n",
    "            num_workers=2, scheduler_method='cos', early_stop_uauc=0.55, label_smoothing=0.2):\n",
    "        \"\"\"\n",
    "        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n",
    "            dictionary mapping input names to Numpy arrays.\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n",
    "        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n",
    "        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n",
    "        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n",
    "        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n",
    "        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n",
    "\n",
    "        :return: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "        \"\"\"\n",
    "        do_validation = False\n",
    "        if validation_data and len(validation_data)==2:\n",
    "            do_validation = True\n",
    "            val_x_loader, val_y = validation_data\n",
    "        else:\n",
    "            val_y = []\n",
    "\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "        if lr_scheduler:\n",
    "            if scheduler_method=='linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                   optim,\n",
    "                   num_warmup_steps=int(len(train_loader)*num_warm_epochs),\n",
    "                   num_training_steps=int(len(train_loader)*(scheduler_epochs)))\n",
    "            else:\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                   optim,\n",
    "                   num_warmup_steps=int(len(train_loader)*num_warm_epochs),\n",
    "                   num_training_steps=int(len(train_loader)*(scheduler_epochs)))\n",
    "        # print('0000 - - reload')\n",
    "        if self.gpus:\n",
    "            print('parallel running on these gpus:', self.gpus)\n",
    "            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n",
    "            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        steps_per_epoch = len(train_loader)\n",
    "\n",
    "        # configure callbacks\n",
    "        callbacks = (callbacks or []) + [self.history]  # add history callback\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.on_train_begin()\n",
    "        callbacks.set_model(self)\n",
    "        if not hasattr(callbacks, 'model'):\n",
    "            callbacks.__setattr__('model', self)\n",
    "        callbacks.model.stop_training = False\n",
    "\n",
    "        # Train\n",
    "        best_metric = -1\n",
    "        early_stopping_flag = False\n",
    "        result_logs = {}\n",
    "        logger.info(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_loader)*train_loader.batch_size, len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            sample_num = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for _, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "                        sample_num += x.shape[0]\n",
    "                        y_pred = model(x)#.squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "                        total_loss = reg_loss + self.aux_loss\n",
    "                        for i in range(self.num_tasks):\n",
    "                            loss = loss_func(y_pred[:,i], y[:,i], reduction=reduction)  # y.squeeze()\n",
    "                            total_loss += loss*task_weight[i]\n",
    "                            loss_epoch += loss.item()\n",
    "                            # 输出日志用\n",
    "                            loss_name = task_dict[i]+'_loss'\n",
    "                            if loss_name not in train_result:\n",
    "                                train_result[loss_name] = []\n",
    "                            train_result[loss_name].append(loss.item())\n",
    "                        \n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward()\n",
    "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                        #torch.nn.utils.clip_grad_value_(model.parameters(), 10.)\n",
    "                        optim.step()\n",
    "                        if lr_scheduler:\n",
    "                            scheduler.step() \n",
    "                        \n",
    "                        # 为节约时间，训练集只输出loss，不做评估\n",
    "                        if False: # verbose>0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                # 训练集不做auc和uauc的评估\n",
    "                                if (name=='uauc'): continue\n",
    "                                if (name=='auc'): continue\n",
    "                                # 每个评估函数都要评估多个任务\n",
    "                                for i in range(model.num_tasks):\n",
    "                                    task_name = task_dict[i]+'_'+name\n",
    "                                    if task_name not in train_result:\n",
    "                                        train_result[task_name] = []\n",
    "                                    try:\n",
    "                                        train_result[task_name].append(metric_fun(\n",
    "                                            y[:,i].squeeze().cpu().data.numpy(), \n",
    "                                            y_pred[i].cpu().data.numpy().astype('float64')))\n",
    "                                    except:\n",
    "                                        pass\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs, training logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / sample_num  # len(result)\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x_loader, val_y, val_userid_list, task_dict)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "                    \n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                eval_str = 'Epoch {0}/{1}'.format(epoch + 1, epochs)\n",
    "\n",
    "                eval_str += \" {0}s - loss: {1: .4f}\".format(epoch_time, epoch_logs[\"loss\"])\n",
    "                \n",
    "                # 输出训练集的评估结果\n",
    "                # for name in self.metrics:\n",
    "                #     if name=='uauc': continue\n",
    "                #     for i in range(self.num_tasks):\n",
    "                #         task_name = task_dict[i]+'_'+name\n",
    "                #         eval_str += \" - \" + task_name + \": {0: .4f}\".format(epoch_logs[task_name])\n",
    "                for name, result in train_result.items():\n",
    "                    eval_str += \" - \" + name + \": {0: .4f}\".format(epoch_logs[name])\n",
    "                \n",
    "                # 输出验证集的评估结果\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        for i in range(self.num_tasks):\n",
    "                            task_name = task_dict[i]+'_'+name\n",
    "                            eval_str += \" - \" + \"val_\" + task_name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + task_name])\n",
    "\n",
    "                    best_metric = max(best_metric, eval_result['UAUC'])\n",
    "                    if best_metric<=early_stop_uauc:\n",
    "                        # 验证集uauc<=0.55时，提前终止训练，以节约时间\n",
    "                        early_stopping_flag = True\n",
    "                    epoch_logs['val_UAUC'] = eval_result['UAUC']\n",
    "                    eval_str += ' - val_UAUC: {0: .5f}'.format(eval_result['UAUC'])\n",
    "                logger.info(eval_str)\n",
    "                \n",
    "            result_logs['epoch'+str(epoch+1)] = epoch_logs\n",
    "            # 验证集uauc==0.5时，提前终止训练，以节约时间\n",
    "            if early_stopping_flag:\n",
    "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "                callbacks.on_train_end()\n",
    "                return self.history, best_metric, result_logs\n",
    "                \n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "        return self.history, best_metric, result_logs\n",
    "\n",
    "    def evaluate(self, x, y, userid_list, task_dict=None):\n",
    "        \"\"\"\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x)\n",
    "        eval_result = {}\n",
    "        eval_result['UAUC'] = 0\n",
    "        weight = [4,3,2,1,1,1,1]\n",
    "        if pred_ans.shape[1]<7:\n",
    "            weight = [4,3,2] if pred_ans.shape[1]==3 else [1,1,1,1]\n",
    "        # 外层多个评估函数\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            # 内层每个评估函数都要对多个任务进行评估\n",
    "            # 生成参数\n",
    "            if name=='uauc':\n",
    "                uauc, task_uaucs = metric_fun(y, pred_ans, userid_list, weight=weight) \n",
    "                eval_result['UAUC'] = uauc\n",
    "                for i in range(self.num_tasks):\n",
    "                    eval_result[task_dict[i]+'_'+name] = task_uaucs[i]\n",
    "            else:\n",
    "                for i in range(self.num_tasks):\n",
    "                    eval_result[task_dict[i]+'_'+name] = metric_fun(y[:,i], pred_ans[:,i])\n",
    "            \n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "\n",
    "        pred_ans = None # np.empty([0, 7])\n",
    "        with torch.no_grad():\n",
    "            for _, (x_test) in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "                y_pred = model(x).cpu().numpy()  # .squeeze()\n",
    "                if pred_ans is None:\n",
    "                    pred_ans = np.empty([0, y_pred.shape[1]])\n",
    "                pred_ans = np.vstack([pred_ans, y_pred])\n",
    "\n",
    "        return pred_ans\n",
    "\n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        # For a Parameter, put it in a list to keep Compatible with get_regularization_loss()\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        # For generators, filters and ParameterLists, convert them to a list of tensors to avoid bugs.\n",
    "        # e.g., we can't pickle generator objects when we save the model.\n",
    "        else:\n",
    "            weight_list = list(weight_list)\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "\n",
    "    def compile(self, optimizer,\n",
    "                learning_rate=0.01,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer, learning_rate)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer, learning_rate):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate)  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters(), lr=learning_rate)  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "            elif optimizer == 'adamw':\n",
    "                optim = torch.optim.AdamW(self.parameters(), lr=learning_rate, weight_decay=0.)\n",
    "            elif optimizer == 'adamax':\n",
    "                optim = torch.optim.Adamax(self.parameters(), lr=learning_rate, weight_decay=0.)\n",
    "            elif optimizer == 'momentum':\n",
    "                optim = torch.optim.SGD(self.parameters(),lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "    \n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                \n",
    "                # 添加uauc metric\n",
    "                if metric == 'uauc':\n",
    "                    metrics_[metric] = uAUC\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "\n",
    "class PretrainedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, init_weight):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(init_weight))\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "\n",
    "class MOE(MyBaseModel):\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, use_fm=False, use_nfm=False,\n",
    "                 dnn_hidden_units=(256, 128), l2_reg_linear=0.00001, l2_reg_embedding=0.00001, \n",
    "                 l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, bi_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=True, task='binary', device='cpu', gpus=None,\n",
    "                 pretrained_embedding_dict=None, frozen_pretrained=False, num_tasks=4, \n",
    "                 pretrained_user_emb_weight=None, pretrained_author_emb_weight=None,\n",
    "                 pretrained_feed_emb_weight=None, pretrained_bgm_song_emb_weight=None,\n",
    "                 pretrained_bgm_singer_emb_weight=None):\n",
    "\n",
    "        super(MOE, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                       l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                       device=device, gpus=gpus, pretrained_embedding_dict=pretrained_embedding_dict, \n",
    "                                       frozen_pretrained=frozen_pretrained, num_tasks=num_tasks)\n",
    "        \n",
    "        dnn_hidden_units = dnn_hidden_units if len(dnn_hidden_units)==num_tasks and isinstance(dnn_hidden_units[0], tuple)\\\n",
    "            else [dnn_hidden_units for i in range(num_tasks)]\n",
    "        self.num_tasks = num_tasks\n",
    "        self.use_fm = use_fm\n",
    "        self.use_nfm = use_nfm\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(dnn_hidden_units) > 0\n",
    "        \n",
    "        self.pretrained_user = pretrained_user_emb_weight is not None\n",
    "        self.pretrained_feed = pretrained_feed_emb_weight is not None\n",
    "        self.pretrained_author = pretrained_author_emb_weight is not None\n",
    "        self.user_emb = []\n",
    "        self.feed_emb = []\n",
    "        self.author_emb = []\n",
    "\n",
    "        self.pretrained_emb_dim = 0\n",
    "        # 载入预训练的的Embedding矩阵\n",
    "        if self.pretrained_user:\n",
    "            for emb_w in pretrained_user_emb_weight:\n",
    "                self.user_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                                         emb_w.shape[1],\n",
    "                                                         emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.user_emb = nn.ModuleList(self.user_emb)\n",
    "        \n",
    "        if self.pretrained_author:\n",
    "            for emb_w in pretrained_author_emb_weight:\n",
    "                self.author_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                                emb_w.shape[1],\n",
    "                                                emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.author_emb = nn.ModuleList(self.author_emb)\n",
    "\n",
    "        if self.pretrained_feed:\n",
    "            for emb_w in pretrained_feed_emb_weight:\n",
    "                self.feed_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                            emb_w.shape[1],\n",
    "                                            emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.feed_emb = nn.ModuleList(self.feed_emb)\n",
    "        \n",
    "        if use_fm:\n",
    "            self.fm = FM()\n",
    "        \n",
    "        dnn_input_dim = self.compute_input_dim(dnn_feature_columns)\n",
    "        dnn_input_dim += self.pretrained_emb_dim\n",
    "        \n",
    "        self.pre_dnn = DNN(dnn_input_dim, (512,), activation=dnn_activation, l2_reg=l2_reg_dnn, \n",
    "                           dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device)\n",
    "        dnn_input_dim = 512\n",
    "        \n",
    "        self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.pre_dnn.named_parameters()), l2=l2_reg_dnn)\n",
    "        \n",
    "        self.dnn = nn.ModuleList([DNN(dnn_input_dim, dnn_hidden_units[i],\n",
    "                       activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                       init_std=init_std, device=device) for i in range(self.num_tasks)])\n",
    "        \n",
    "        # 相关性较强的几个任务, dnn最后一层的输出可以和相关任务输入合并输入最后一层\n",
    "        # [read_comment, like, click_avatar, forward, favorite, comment, follow]\n",
    "        # read_comment & comment / comment = 0.5585950692333671 => comment 可以作为 read_comment 的输入\n",
    "        # like & comment / comment = 0.34886862546437014        => comment 可以作为like的输入\n",
    "        # click_avatar & follow / follow = 0.9833238582527951   => follow可以作为 click_avatar的输入\n",
    "        dnn_linear = []\n",
    "        for i in range(self.num_tasks):\n",
    "            if (i==0) or (i==1) or (i==2):\n",
    "                dnn_linear.append(nn.Linear(dnn_hidden_units[i][-1]*2, 1, bias=False))\n",
    "            else:\n",
    "                dnn_linear.append(nn.Linear(dnn_hidden_units[i][-1], 1, bias=False))\n",
    "\n",
    "        self.dnn_linear = nn.ModuleList(dnn_linear).to(device)\n",
    "\n",
    "        for task_dnn in self.dnn:\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], task_dnn.named_parameters()), l2=l2_reg_dnn)\n",
    "        for task_out_linear in self.dnn_linear:\n",
    "            self.add_regularization_weight(task_out_linear.weight, l2=l2_reg_dnn)\n",
    "                \n",
    "        self.out = nn.ModuleList([PredictionLayer(task, ) for i in range(self.num_tasks)])\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "\n",
    "        pretrained_emb = []\n",
    "        # 预训练Embedding\n",
    "        if self.pretrained_user:\n",
    "            for emb in self.user_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['userid'][0]].long()))\n",
    "        if self.pretrained_author:\n",
    "            for emb in self.author_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['authorid'][0]].long()))\n",
    "        if self.pretrained_feed:\n",
    "            for emb in self.feed_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['feedid'][0]].long()))\n",
    "\n",
    "\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "        dnn_input = torch.cat([dnn_input]+pretrained_emb, dim=1)\n",
    "        \n",
    "        dnn_input = self.pre_dnn(dnn_input)\n",
    "        \n",
    "        dnn_outputs = []\n",
    "        for task_dnn in self.dnn:\n",
    "            dnn_output = task_dnn(dnn_input)\n",
    "            dnn_outputs.append(dnn_output)\n",
    "        \n",
    "        # 相关性较强的几个任务, dnn最后一层的输出可以和相关任务输入合并输入最后一层\n",
    "        # [read_comment, like, click_avatar, forward, favorite, comment, follow]\n",
    "        # read_comment & comment / comment = 0.5585950692333671 => comment 可以作为 read_comment 的输入\n",
    "        # like & comment / comment = 0.34886862546437014        => comment 可以作为like的输入\n",
    "        # click_avatar & follow / follow = 0.9833238582527951   => follow可以作为 click_avatar的输入\n",
    "        dnn_outputs[0] = torch.cat([dnn_outputs[0], dnn_outputs[5]], dim=1)\n",
    "        dnn_outputs[1] = torch.cat([dnn_outputs[1], dnn_outputs[5]], dim=1)\n",
    "        dnn_outputs[2] = torch.cat([dnn_outputs[2], dnn_outputs[6]], dim=1)\n",
    "        \n",
    "        logit = []\n",
    "        i = 0\n",
    "        for task_dnn_linear, task_out in zip(self.dnn_linear, self.out):\n",
    "            dnn_logit = task_dnn_linear(dnn_outputs[i])\n",
    "            logit.append(task_out(dnn_logit))\n",
    "            i += 1\n",
    "        # print(torch.cat(logit, dim=1).shape)\n",
    "        return torch.cat(logit, dim=1)# from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa7c7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812 512\n",
      "512 256\n",
      "256 128\n"
     ]
    }
   ],
   "source": [
    "0: 'read_comment',\n",
    "1: 'like',\n",
    "2: 'click_avatar',\n",
    "3: 'forward',\n",
    "4: 'favorite',\n",
    "5: 'comment',\n",
    "6: 'follow'\n",
    "# 相关性较强的几个任务：\n",
    "read_comment & click_avatar / click_avatar = 0.10383108402263823\n",
    "read_comment & comment / comment = 0.5585950692333671\n",
    "read_comment & favorite / favorite = 0.11024022801302931\n",
    "\n",
    "like & forward / forward = 0.13628723670564674\n",
    "like & favorite / favorite = 0.1438314332247557\n",
    "like & comment / comment = 0.34886862546437014\n",
    "like & follow / follow = 0.1290505969300739\n",
    "\n",
    "\n",
    "click_avatar & follow / follow = 0.9833238582527951\n",
    "\n",
    "\n",
    "#self.linears = nn.ModuleList(\n",
    "#            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "# hidden_units = [812, 512, 256, 128]\n",
    "# for i in range(len(hidden_units)-1):\n",
    "#     print(hidden_units[i], hidden_units[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a05404d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, pickle\n",
    "sys.path.append('../')\n",
    "import preprocess\n",
    "# from model.moe import MOE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f46660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../my_data/w2v_models_sg_ns_64_epoch30//userid_by_feedid_w10_iter10.64d.pkl\n",
      "classes numbers:  219999\n",
      "word2vec vocab size:  219999\n",
      "Total Random initialized word embedding counts:  0\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//userid_by_authorid_w10_iter10.64d.pkl\n",
      "classes numbers:  219999\n",
      "word2vec vocab size:  219999\n",
      "Total Random initialized word embedding counts:  0\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//authorid_w7_iter10.64d.pkl\n",
      "classes numbers:  18789\n",
      "word2vec vocab size:  18788\n",
      "Total Random initialized word embedding counts:  1\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//feedid_w7_iter10.64d.pkl\n",
      "classes numbers:  106444\n",
      "word2vec vocab size:  103864\n",
      "Total Random initialized word embedding counts:  2580\n",
      "../my_data/official_feed_emb.d512.pkl\n",
      "classes numbers:  106444\n",
      "word2vec vocab size:  106444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2021 17:45:04 - INFO - __main__ -   All used features:\n",
      "08/03/2021 17:45:04 - INFO - __main__ -   dict_keys(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'videoplayseconds_bin', 'bgm_na', 'feed_machine_tag_tfidf_cls_32', 'feed_machine_kw_tfidf_cls_17', 'author_machine_tag_tfidf_cls_21', 'author_machine_kw_tfidf_cls_18', 'videoplayseconds', 'tag_manu_machine_corr'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Random initialized word embedding counts:  0\n"
     ]
    }
   ],
   "source": [
    "W2V_SG_EPOCH20 = '../my_data/w2v_models_sg_ns_64_epoch20/'\n",
    "W2V_SG_EPOCH30 = '../my_data/w2v_models_sg_ns_64_epoch30/'\n",
    "W2V_SG_EPOCH40 = '../my_data/w2v_models_sg_ns_64_epoch40/'\n",
    "\n",
    "W2V_CBOW_EPOCH20 = '../my_data/w2v_models_cbow_ns_64_epoch20/'\n",
    "W2V_CBOW_EPOCH30 = '../my_data/w2v_models_cbow_ns_64_epoch30/'\n",
    "\n",
    "TOPIC_COLS = ['feed_manu_tag_topic_class', 'feed_machine_tag_topic_class', 'feed_manu_kw_topic_class', \n",
    "              'feed_machine_kw_topic_class', 'feed_description_topic_class', 'author_description_topic_class', \n",
    "              'author_manu_kw_topic_class', 'author_machine_kw_topic_class', 'author_manu_tag_topic_class', \n",
    "              'author_machine_tag_topic_class']\n",
    "\n",
    "pretrained_models = {\n",
    "    'sg_ns_64_epoch20':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_SG_EPOCH20}/feedid_w7_iter10.64d.pkl',\n",
    "        'feed_description_tfidf_cls_18':f'{W2V_SG_EPOCH20}/feed_description_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'feed_machine_kw_tfidf_cls_17':f'{W2V_SG_EPOCH20}/feed_machine_kw_tfidf_cls_17_w7_iter10.64d.pkl',\n",
    "        'feed_machine_tag_tfidf_cls_32':f'{W2V_SG_EPOCH20}/feed_machine_tag_tfidf_cls_32_w7_iter10.64d.pkl',\n",
    "        'feed_manu_kw_tfidf_cls_22':f'{W2V_SG_EPOCH20}/feed_manu_kw_tfidf_cls_22_w7_iter10.64d.pkl',\n",
    "        'feed_manu_tag_tfidf_cls_32':f'{W2V_SG_EPOCH20}/feed_manu_tag_tfidf_cls_32_w7_iter10.64d.pkl',\n",
    "\n",
    "        'authorid': f'{W2V_SG_EPOCH20}/authorid_w7_iter10.64d.pkl',\n",
    "        'author_description_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_description_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_machine_kw_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_machine_kw_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_machine_tag_tfidf_cls_21':f'{W2V_SG_EPOCH20}/author_machine_tag_tfidf_cls_21_w7_iter10.64d.pkl',\n",
    "        'author_manu_kw_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_manu_kw_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_manu_tag_tfidf_cls_19':f'{W2V_SG_EPOCH20}/author_manu_tag_tfidf_cls_19_w7_iter10.64d.pkl',\n",
    "\n",
    "        'userid_by_feed': f'{W2V_SG_EPOCH20}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_SG_EPOCH20}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    },\n",
    "    'sg_ns_64_epoch30':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_SG_EPOCH30}/feedid_w7_iter10.64d.pkl',\n",
    "        'authorid': f'{W2V_SG_EPOCH30}/authorid_w7_iter10.64d.pkl',\n",
    "        'userid_by_feed': f'{W2V_SG_EPOCH30}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_SG_EPOCH30}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    }\n",
    "}\n",
    "\n",
    "USED_FEATURES = ['userid','feedid','authorid','bgm_song_id','bgm_singer_id','videoplayseconds_bin','bgm_na',\n",
    "                 'videoplayseconds','tag_manu_machine_corr']+\\\n",
    "                ['feed_machine_tag_tfidf_cls_32','feed_machine_kw_tfidf_cls_17',\n",
    "                 'author_machine_tag_tfidf_cls_21','author_machine_kw_tfidf_cls_18']\n",
    "\n",
    "DATA_PATH = '../my_data/data_base/'\n",
    "\n",
    "args = {}\n",
    "args['USED_FEATURES'] = USED_FEATURES\n",
    "args['DATA_PATH'] = DATA_PATH\n",
    "\n",
    "global hidden_units\n",
    "hidden_units = (512,256,128)\n",
    "args['hidden_units'] = hidden_units\n",
    "args['batch_size'] = 40000\n",
    "args['emb_dim'] = 16\n",
    "args['learning_rate'] = 0.05\n",
    "args['lr_scheduler'] = True\n",
    "args['epochs'] = 2\n",
    "args['scheduler_epochs'] = 3\n",
    "args['num_warm_epochs'] = 0\n",
    "args['scheduler_method'] = 'cos'\n",
    "args['use_bn'] = True\n",
    "args['reduction'] = 'sum'\n",
    "args['optimizer'] = 'adagrad'\n",
    "args['num_tasks'] = 7\n",
    "args['early_stop_uauc'] = 0.689\n",
    "args['num_workers'] = 7\n",
    "args['task_dict'] = {\n",
    "        0: 'read_comment',\n",
    "        1: 'like',\n",
    "        2: 'click_avatar',\n",
    "        3: 'forward',\n",
    "        4: 'favorite',\n",
    "        5: 'comment',\n",
    "        6: 'follow'\n",
    "}\n",
    "args['task_weight'] = {\n",
    "        0: 1,\n",
    "        1: 1,\n",
    "        2: 1,\n",
    "        3: 1,\n",
    "        4: 1,\n",
    "        5: 1,\n",
    "        6: 1\n",
    "}\n",
    "args['opt_iters'] = [10, 10]\n",
    "args['pbounds'] = {'dropout': (0.0, 0.9),\n",
    "                   #'learning_rate': 0.001,\n",
    "                   'l2_reg_dnn': (0.0001,0.0001),\n",
    "                   'l2_reg_embedding': (0.1, 0.1),\n",
    "                   'l2_reg_linear': (0.1, 0.1)\n",
    "                  }\n",
    "\n",
    "args['pretrained_model'] = pretrained_models['sg_ns_64_epoch30']\n",
    "\n",
    "\n",
    "# 全部特征\n",
    "linear_feature_columns = pickle.load(open(DATA_PATH+'/linear_feature.pkl','rb'))\n",
    "dnn_feature_columns = pickle.load(open(DATA_PATH+'/dnn_feature.pkl','rb'))\n",
    "#print('raw:')\n",
    "#print(dnn_feature_columns)\n",
    "# 使用其中部分特征\n",
    "linear_feature_columns = [f for f in linear_feature_columns if f.name in USED_FEATURES]\n",
    "dnn_feature_columns = [f for f in dnn_feature_columns if f.name in USED_FEATURES]\n",
    "features = []\n",
    "for f in linear_feature_columns:\n",
    "    if isinstance(f, SparseFeat):\n",
    "        features.append(SparseFeat(f.name, f.vocabulary_size, args['emb_dim']))\n",
    "    else:\n",
    "        features.append(f)\n",
    "linear_feature_columns = features\n",
    "dnn_feature_columns = features\n",
    "\n",
    "lbe_dict = preprocess.LBE_MODEL\n",
    "\n",
    "# pri_train_X = pickle.load(open(DATA_PATH+'/pri_train_x.pkl','rb'))\n",
    "# pri_train_y = pickle.load(open(DATA_PATH+'/pri_train_y.pkl','rb'))\n",
    "# pri_val_X = pickle.load(open(DATA_PATH+'/pri_val_x.pkl','rb'))\n",
    "# pri_val_y = pickle.load(open(DATA_PATH+'/pri_val_y.pkl','rb'))\n",
    "\n",
    "semi_train_X = pickle.load(open(DATA_PATH+'/semi_train_x.pkl','rb'))\n",
    "semi_train_y = pickle.load(open(DATA_PATH+'/semi_train_y.pkl','rb'))\n",
    "semi_val_X = pickle.load(open(DATA_PATH+'/semi_val_x.pkl','rb'))\n",
    "semi_val_y = pickle.load(open(DATA_PATH+'/semi_val_y.pkl','rb'))\n",
    "# 从数据集中选取部分特征\n",
    "semi_train_X = {f.name:semi_train_X[f.name] for f in dnn_feature_columns}\n",
    "semi_val_X = {f.name:semi_val_X[f.name] for f in dnn_feature_columns}\n",
    "# pri_train_X = {f.name:pri_train_X[f.name] for f in dnn_feature_columns}\n",
    "# pri_val_X = {f.name:pri_val_X[f.name] for f in dnn_feature_columns}\n",
    "\n",
    "# for col in semi_train_X:\n",
    "#     semi_train_X[col] = np.concatenate((semi_train_X[col], pri_train_X[col]), axis=0)\n",
    "# semi_train_y = np.concatenate((semi_train_y, pri_train_y), axis=0)\n",
    "\n",
    "lbe_dict = preprocess.LBE_MODEL\n",
    "# 载入预训练Embedding weight matrix\n",
    "user_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['userid'], \n",
    "                                                    args['pretrained_model']['userid_by_feed'], padding=True)\n",
    "# user_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['userid'], \n",
    "#                                    '../../src/my_data/eges/user_deepwalk_w2v_w10.64d.pkl', padding=True)\n",
    "user_by_author_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['userid'], \n",
    "                                                    args['pretrained_model']['userid_by_author'], padding=True)\n",
    "\n",
    "author_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['authorid'], \n",
    "                                                    args['pretrained_model']['authorid'], padding=True)\n",
    "feed_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['feedid'], \n",
    "                                                    args['pretrained_model']['feedid'], padding=True)\n",
    "\n",
    "# feed_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['feedid'], \n",
    "#                                                     '../my_data/eges/feedid_eges2_emb.pkl', padding=True)\n",
    "\n",
    "official_feed_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['feedid'], \n",
    "                                                    args['pretrained_model']['official_feed'], padding=True)\n",
    "\n",
    "logger.info('All used features:')\n",
    "logger.info(semi_train_X.keys())\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ff2c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'videoplayseconds_bin', 'bgm_na', 'feed_machine_tag_tfidf_cls_32', 'feed_machine_kw_tfidf_cls_17', 'author_machine_tag_tfidf_cls_21', 'author_machine_kw_tfidf_cls_18', 'videoplayseconds', 'tag_manu_machine_corr'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "_moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=0.,\n",
    "          l2_reg_embedding=0., l2_reg_dnn=0.,\n",
    "          l2_reg_linear=0., device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=None,\n",
    "          pretrained_author_emb_weight=None,\n",
    "          pretrained_feed_emb_weight=None,\n",
    "          )\n",
    "\n",
    "train_loader = preprocess.get_dataloader(semi_train_X, _moe, y=semi_train_y, batch_size=args['batch_size'],  \n",
    "                   num_workers=10)\n",
    "\n",
    "val_tt_loader = preprocess.get_dataloader(semi_val_X, _moe, y=semi_val_y, batch_size=args['batch_size'],  \n",
    "                   num_workers=10)\n",
    "\n",
    "val_loader = preprocess.get_dataloader(semi_val_X, _moe, y=None, batch_size=args['batch_size'],  \n",
    "                   num_workers=10)\n",
    "\n",
    "# # 前3个任务\n",
    "# train_loader3 = preprocess.get_dataloader(semi_train_X, _moe, y=semi_train_y[:,:3], batch_size=args['batch_size'],  \n",
    "#                    num_workers=10)\n",
    "\n",
    "# val_tt_loader3 = preprocess.get_dataloader(semi_val_X, _moe, y=semi_val_y[:,:3], batch_size=args['batch_size'],  \n",
    "#                    num_workers=10)\n",
    "\n",
    "\n",
    "# # 后4个任务\n",
    "# train_loader4 = preprocess.get_dataloader(semi_train_X, _moe, y=semi_train_y[:,3:], batch_size=args['batch_size'],  \n",
    "#                    num_workers=10)\n",
    "\n",
    "# val_tt_loader4 = preprocess.get_dataloader(semi_val_X, _moe, y=semi_val_y[:,3:], batch_size=args['batch_size'],  \n",
    "#                    num_workers=10)\n",
    "\n",
    "\n",
    "val_userid_lst = semi_val_X['userid'].tolist()\n",
    "\n",
    "semi_train_X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c84c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'target': 0.6984976583814221, 'params': \n",
    "# params = {'dropout': 0.08746740041525639,\n",
    "#           'l2_reg_dnn': 0.0001,\n",
    "#           'l2_reg_embedding': 0.05032424704698356,\n",
    "#           'l2_reg_linear': 0.06932797233659868}\n",
    "\n",
    "# 0.7      |  0.04211  |  0.000423 |  0.06703  |  0.1486\n",
    "# params = {'dropout': 0.04211,\n",
    "#           'l2_reg_dnn': 0.000423,\n",
    "#           'l2_reg_embedding': 0.06703,\n",
    "#           'l2_reg_linear': 0.1486}\n",
    "\n",
    "# 仅复赛数据   线上 0.702389，当前最高单模\n",
    "#'target': 0.6991456900580608\n",
    "# params =  {'dropout': 0.0, 'l2_reg_dnn': 0.001, 'l2_reg_embedding': 0.01, 'l2_reg_linear': 0.01}\n",
    "\n",
    "params =  {'dropout': 0.0, 'l2_reg_dnn': 0.001, 'l2_reg_embedding': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133f5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# moe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f01cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2021 17:48:20 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "0it [00:04, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [40000 x 512], m2: [882 x 512] at /tmp/pip-req-build-irc6u2ci/aten/src/THC/generic/THCTensorMathBlas.cu:290",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-694a897c5585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                    \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reduction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                    \u001b[0mtask_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                    early_stop_uauc=0.55)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# lr0.03 0.69847\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3e0fbf9e6080>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, epochs, verbose, initial_epoch, validation_split, validation_data, val_userid_list, shuffle, callbacks, num_warm_epochs, lr_scheduler, scheduler_epochs, reduction, task_weight, task_dict, num_workers, scheduler_method, early_stop_uauc, label_smoothing)\u001b[0m\n\u001b[1;32m    343\u001b[0m                         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0msample_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3e0fbf9e6080>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mdnn_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtask_dnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_dnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0mdnn_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-3e0fbf9e6080>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [40000 x 512], m2: [882 x 512] at /tmp/pip-req-build-irc6u2ci/aten/src/THC/generic/THCTensorMathBlas.cu:290"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "# 不要FM 和 linear\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'], \n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'], \n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)\n",
    "\n",
    "# lr0.03 0.69847\n",
    "# lr0.05 0.70000\n",
    "# lr0.06 \n",
    "# lr0.075 0.70027 \n",
    "# lr0.8 \n",
    "# lr0.09 \n",
    "# lr0.1  0.69965 \n",
    "\n",
    "# epoch0   0.69603\n",
    "# epoch1   0.69507\n",
    "# epoch2   0.69571\n",
    "\n",
    "# lr0.05  val_UAUC:  0.68665\n",
    "# lr0.1   val_UAUC:  0.68718\n",
    "# lr0.075 val_UAUC:  0.68755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e33636d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2021 21:04:24 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "1647it [06:43,  4.08it/s]\n",
      "08/02/2021 21:12:19 - INFO - __main__ -   Epoch 1/3 475s - loss:  0.0385 - read_comment_loss:  0.0200 - like_loss:  0.0068 - click_avatar_loss:  0.0037 - forward_loss:  0.0055 - val_read_comment_binary_crossentropy:  0.0183 - val_like_binary_crossentropy:  0.0064 - val_click_avatar_binary_crossentropy:  0.0028 - val_forward_binary_crossentropy:  0.0050 - val_read_comment_auc:  0.9002 - val_like_auc:  0.9437 - val_click_avatar_auc:  0.8959 - val_forward_auc:  0.8971 - val_read_comment_uauc:  0.7454 - val_like_uauc:  0.7697 - val_click_avatar_uauc:  0.6176 - val_forward_uauc:  0.7348 - val_UAUC:  0.71687\n",
      "1647it [06:31,  4.21it/s]\n",
      "08/02/2021 21:20:04 - INFO - __main__ -   Epoch 2/3 464s - loss:  0.0317 - read_comment_loss:  0.0175 - like_loss:  0.0053 - click_avatar_loss:  0.0024 - forward_loss:  0.0042 - val_read_comment_binary_crossentropy:  0.0183 - val_like_binary_crossentropy:  0.0063 - val_click_avatar_binary_crossentropy:  0.0028 - val_forward_binary_crossentropy:  0.0049 - val_read_comment_auc:  0.9039 - val_like_auc:  0.9432 - val_click_avatar_auc:  0.9066 - val_forward_auc:  0.9048 - val_read_comment_uauc:  0.7481 - val_like_uauc:  0.7727 - val_click_avatar_uauc:  0.6299 - val_forward_uauc:  0.7454 - val_UAUC:  0.72402\n",
      "1647it [06:32,  4.20it/s]\n",
      "08/02/2021 21:27:50 - INFO - __main__ -   Epoch 3/3 466s - loss:  0.0289 - read_comment_loss:  0.0163 - like_loss:  0.0049 - click_avatar_loss:  0.0021 - forward_loss:  0.0037 - val_read_comment_binary_crossentropy:  0.0189 - val_like_binary_crossentropy:  0.0067 - val_click_avatar_binary_crossentropy:  0.0029 - val_forward_binary_crossentropy:  0.0053 - val_read_comment_auc:  0.8985 - val_like_auc:  0.9314 - val_click_avatar_auc:  0.9004 - val_forward_auc:  0.8962 - val_read_comment_uauc:  0.7439 - val_like_uauc:  0.7628 - val_click_avatar_uauc:  0.6260 - val_forward_uauc:  0.7398 - val_UAUC:  0.71809\n"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "# 不要FM 和 linear\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=4, #args['num_tasks'], \n",
    "          pretrained_user_emb_weight=[user_emb_weight], \n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.075, loss=\"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader4, validation_data=[val_loader, semi_val_y[:,3:]],\n",
    "                   epochs=3, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)\n",
    "\n",
    "\n",
    "# lr0.03 0.69847\n",
    "# lr0.05 0.70000\n",
    "# lr0.06 \n",
    "# lr0.075 0.70027 \n",
    "# lr0.8 \n",
    "# lr0.09 \n",
    "# lr0.1  0.69965 \n",
    "\n",
    "\n",
    "# epoch0   0.69603\n",
    "# epoch1   0.69507\n",
    "# epoch2   0.69571\n",
    "\n",
    "# lr0.05   0.7206\n",
    "# lr0.075  0.72402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa6b696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2021 21:27:51 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "1647it [06:45,  4.06it/s]\n",
      "08/02/2021 21:35:51 - INFO - __main__ -   Epoch 1/3 479s - loss:  0.0387 - read_comment_loss:  0.0200 - like_loss:  0.0068 - click_avatar_loss:  0.0036 - forward_loss:  0.0054 - val_read_comment_binary_crossentropy:  0.0184 - val_like_binary_crossentropy:  0.0064 - val_click_avatar_binary_crossentropy:  0.0028 - val_forward_binary_crossentropy:  0.0050 - val_read_comment_auc:  0.8986 - val_like_auc:  0.9436 - val_click_avatar_auc:  0.8924 - val_forward_auc:  0.8946 - val_read_comment_uauc:  0.7443 - val_like_uauc:  0.7685 - val_click_avatar_uauc:  0.6177 - val_forward_uauc:  0.7338 - val_UAUC:  0.71607\n",
      "1347it [05:20,  4.25it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "# 不要FM 和 linear\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=4, #args['num_tasks'], \n",
    "          pretrained_user_emb_weight=[user_emb_weight], \n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.1, loss=\"binary_crossentropy\",\n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader4, validation_data=[val_loader, semi_val_y[:,3:]],\n",
    "                   epochs=3, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)\n",
    "\n",
    "\n",
    "# lr0.03 0.69847\n",
    "# lr0.05 0.70000\n",
    "# lr0.06 \n",
    "# lr0.075 0.70027 \n",
    "# lr0.8 \n",
    "# lr0.09 \n",
    "# lr0.1  0.69965 \n",
    "\n",
    "\n",
    "# epoch0   0.69603\n",
    "# epoch1   0.69507\n",
    "# epoch2   0.69571\n",
    "\n",
    "# lr0.05   0.7206\n",
    "# lr0.075  0.72402\n",
    "# lr0.1    0.723"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c32e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01d1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2307e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/29/2021 18:19:07 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "1647it [11:17,  2.43it/s]\n",
      "07/29/2021 18:32:30 - INFO - __main__ -   Epoch 1/2 802s - loss:  0.0778 - read_comment_loss:  0.0895 - like_loss:  0.0900 - click_avatar_loss:  0.0359 - forward_loss:  0.0205 - favorite_loss:  0.0073 - comment_loss:  0.0044 - follow_loss:  0.0056 - val_read_comment_binary_crossentropy:  0.0900 - val_like_binary_crossentropy:  0.0893 - val_click_avatar_binary_crossentropy:  0.0357 - val_forward_binary_crossentropy:  0.0187 - val_favorite_binary_crossentropy:  0.0066 - val_comment_binary_crossentropy:  0.0033 - val_follow_binary_crossentropy:  0.0051 - val_read_comment_auc:  0.9345 - val_like_auc:  0.8598 - val_click_avatar_auc:  0.8752 - val_forward_auc:  0.8923 - val_favorite_auc:  0.9386 - val_comment_auc:  0.8325 - val_follow_auc:  0.8888 - val_read_comment_uauc:  0.6652 - val_like_uauc:  0.6631 - val_click_avatar_uauc:  0.7535 - val_forward_uauc:  0.7393 - val_favorite_uauc:  0.7639 - val_comment_uauc:  0.6224 - val_follow_uauc:  0.7408 - val_UAUC:  0.69414\n",
      "1647it [10:53,  2.52it/s]\n",
      "07/29/2021 18:45:29 - INFO - __main__ -   Epoch 2/2 779s - loss:  0.0728 - read_comment_loss:  0.0849 - like_loss:  0.0863 - click_avatar_loss:  0.0335 - forward_loss:  0.0180 - favorite_loss:  0.0056 - comment_loss:  0.0034 - follow_loss:  0.0048 - val_read_comment_binary_crossentropy:  0.0890 - val_like_binary_crossentropy:  0.0886 - val_click_avatar_binary_crossentropy:  0.0354 - val_forward_binary_crossentropy:  0.0184 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0033 - val_follow_binary_crossentropy:  0.0050 - val_read_comment_auc:  0.9374 - val_like_auc:  0.8626 - val_click_avatar_auc:  0.8790 - val_forward_auc:  0.9003 - val_favorite_auc:  0.9431 - val_comment_auc:  0.8390 - val_follow_auc:  0.8953 - val_read_comment_uauc:  0.6712 - val_like_uauc:  0.6651 - val_click_avatar_uauc:  0.7547 - val_forward_uauc:  0.7443 - val_favorite_uauc:  0.7733 - val_comment_uauc:  0.6209 - val_follow_uauc:  0.7458 - val_UAUC:  0.69797\n"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],early_stop_uauc=0.55,verbose=1,\n",
    "                   task_dict=args['task_dict'], task_weight={0:0.4, 1:0.3, 2:0.2, 3:0.1, 4:0.1, 5:0.1, 6:0.1}\n",
    "                   )\n",
    "\n",
    "# 0:0.4, 1:0.3, 2:0.2, 3:0.1, 4:0.1, 5:0.1, 6:0.1     0.69797\n",
    "# 0:0.5, 1:0.5, 2:0.4, 3:0.1, 4:0.1, 5:0.1, 6:0.1     0.69684\n",
    "# 0:0.2, 1:0.3, 2:0.4, 3:1, 4:1, 5:1, 6:1             0.69883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf09fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/29/2021 18:45:31 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "1647it [11:18,  2.43it/s]\n",
      "07/29/2021 18:58:55 - INFO - __main__ -   Epoch 1/2 804s - loss:  0.1131 - read_comment_loss:  0.0894 - like_loss:  0.0899 - click_avatar_loss:  0.0357 - forward_loss:  0.0205 - favorite_loss:  0.0074 - comment_loss:  0.0045 - follow_loss:  0.0057 - val_read_comment_binary_crossentropy:  0.0900 - val_like_binary_crossentropy:  0.0892 - val_click_avatar_binary_crossentropy:  0.0355 - val_forward_binary_crossentropy:  0.0187 - val_favorite_binary_crossentropy:  0.0066 - val_comment_binary_crossentropy:  0.0034 - val_follow_binary_crossentropy:  0.0051 - val_read_comment_auc:  0.9347 - val_like_auc:  0.8606 - val_click_avatar_auc:  0.8779 - val_forward_auc:  0.8930 - val_favorite_auc:  0.9354 - val_comment_auc:  0.8180 - val_follow_auc:  0.8867 - val_read_comment_uauc:  0.6655 - val_like_uauc:  0.6631 - val_click_avatar_uauc:  0.7545 - val_forward_uauc:  0.7398 - val_favorite_uauc:  0.7634 - val_comment_uauc:  0.6077 - val_follow_uauc:  0.7363 - val_UAUC:  0.69290\n",
      "1647it [10:53,  2.52it/s]\n",
      "07/29/2021 19:11:55 - INFO - __main__ -   Epoch 2/2 779s - loss:  0.1059 - read_comment_loss:  0.0844 - like_loss:  0.0865 - click_avatar_loss:  0.0329 - forward_loss:  0.0183 - favorite_loss:  0.0056 - comment_loss:  0.0035 - follow_loss:  0.0048 - val_read_comment_binary_crossentropy:  0.0891 - val_like_binary_crossentropy:  0.0887 - val_click_avatar_binary_crossentropy:  0.0353 - val_forward_binary_crossentropy:  0.0184 - val_favorite_binary_crossentropy:  0.0065 - val_comment_binary_crossentropy:  0.0034 - val_follow_binary_crossentropy:  0.0051 - val_read_comment_auc:  0.9373 - val_like_auc:  0.8622 - val_click_avatar_auc:  0.8794 - val_forward_auc:  0.9006 - val_favorite_auc:  0.9420 - val_comment_auc:  0.8239 - val_follow_auc:  0.8919 - val_read_comment_uauc:  0.6704 - val_like_uauc:  0.6640 - val_click_avatar_uauc:  0.7557 - val_forward_uauc:  0.7435 - val_favorite_uauc:  0.7743 - val_comment_uauc:  0.6164 - val_follow_uauc:  0.7398 - val_UAUC:  0.69684\n"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],early_stop_uauc=0.55,verbose=1,\n",
    "                   task_dict=args['task_dict'], task_weight={0:0.5, 1:0.5, 2:0.4, 3:0.1, 4:0.1, 5:0.1, 6:0.1}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7499363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/29/2021 19:11:57 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "1647it [11:17,  2.43it/s]\n",
      "07/29/2021 19:25:22 - INFO - __main__ -   Epoch 1/2 804s - loss:  0.0999 - read_comment_loss:  0.0898 - like_loss:  0.0899 - click_avatar_loss:  0.0357 - forward_loss:  0.0200 - favorite_loss:  0.0069 - comment_loss:  0.0037 - follow_loss:  0.0054 - val_read_comment_binary_crossentropy:  0.0903 - val_like_binary_crossentropy:  0.0893 - val_click_avatar_binary_crossentropy:  0.0356 - val_forward_binary_crossentropy:  0.0184 - val_favorite_binary_crossentropy:  0.0063 - val_comment_binary_crossentropy:  0.0028 - val_follow_binary_crossentropy:  0.0049 - val_read_comment_auc:  0.9341 - val_like_auc:  0.8602 - val_click_avatar_auc:  0.8770 - val_forward_auc:  0.9009 - val_favorite_auc:  0.9476 - val_comment_auc:  0.9033 - val_follow_auc:  0.9047 - val_read_comment_uauc:  0.6630 - val_like_uauc:  0.6627 - val_click_avatar_uauc:  0.7542 - val_forward_uauc:  0.7441 - val_favorite_uauc:  0.7723 - val_comment_uauc:  0.6354 - val_follow_uauc:  0.7433 - val_UAUC:  0.69566\n",
      "1647it [10:52,  2.52it/s]\n",
      "07/29/2021 19:38:21 - INFO - __main__ -   Epoch 2/2 779s - loss:  0.0893 - read_comment_loss:  0.0856 - like_loss:  0.0860 - click_avatar_loss:  0.0329 - forward_loss:  0.0174 - favorite_loss:  0.0053 - comment_loss:  0.0025 - follow_loss:  0.0042 - val_read_comment_binary_crossentropy:  0.0894 - val_like_binary_crossentropy:  0.0886 - val_click_avatar_binary_crossentropy:  0.0353 - val_forward_binary_crossentropy:  0.0185 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0028 - val_follow_binary_crossentropy:  0.0049 - val_read_comment_auc:  0.9367 - val_like_auc:  0.8629 - val_click_avatar_auc:  0.8802 - val_forward_auc:  0.8993 - val_favorite_auc:  0.9457 - val_comment_auc:  0.9105 - val_follow_auc:  0.9089 - val_read_comment_uauc:  0.6672 - val_like_uauc:  0.6648 - val_click_avatar_uauc:  0.7578 - val_forward_uauc:  0.7449 - val_favorite_uauc:  0.7729 - val_comment_uauc:  0.6382 - val_follow_uauc:  0.7502 - val_UAUC:  0.69883\n"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(use_fm=False, dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],early_stop_uauc=0.55,verbose=1,\n",
    "                   task_dict=args['task_dict'], task_weight={0:0.2, 1:0.3, 2:0.4, 3:1, 4:1, 5:1, 6:1}\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789f71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a244667",
   "metadata": {},
   "outputs": [],
   "source": [
    "_moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=0.,\n",
    "          l2_reg_embedding=0., l2_reg_dnn=0.,\n",
    "          l2_reg_linear=0., device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=None,\n",
    "          pretrained_author_emb_weight=None,\n",
    "          pretrained_feed_emb_weight=None,\n",
    "          )\n",
    "\n",
    "# 用于线上预测的训练集\n",
    "online_train_X = {}\n",
    "for col in semi_train_X:\n",
    "    online_train_X[col] = np.concatenate((semi_train_X[col], semi_val_X[col]), axis=0)\n",
    "online_train_y = np.concatenate((semi_train_y, semi_val_y), axis=0)\n",
    "\n",
    "online_train_loader = preprocess.get_dataloader(online_train_X, _moe, y=online_train_y, \n",
    "                                              batch_size=args['batch_size'],  \n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7101af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 22:54:50 - INFO - __main__ -   Train on 72000000 samples, validate on 0 samples, 1800 steps per epoch\n",
      "1800it [12:24,  2.42it/s]\n",
      "07/27/2021 23:07:24 - INFO - __main__ -   Epoch 1/2 754s - loss:  0.2609 - read_comment_loss:  0.0889 - like_loss:  0.0896 - click_avatar_loss:  0.0355 - forward_loss:  0.0198 - favorite_loss:  0.0069 - comment_loss:  0.0040 - follow_loss:  0.0055\n",
      "1800it [12:23,  2.42it/s]\n",
      "07/27/2021 23:19:59 - INFO - __main__ -   Epoch 2/2 754s - loss:  0.2388 - read_comment_loss:  0.0834 - like_loss:  0.0848 - click_avatar_loss:  0.0323 - forward_loss:  0.0173 - favorite_loss:  0.0052 - comment_loss:  0.0027 - follow_loss:  0.0044\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          l2_reg_linear=params['l2_reg_linear'], device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight],\n",
    "          )\n",
    "# args['learning_rate']\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc'])\n",
    "\n",
    "metric = moe.fit(online_train_loader, validation_data=None, \n",
    "                   epochs=2, val_userid_list=None, \n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'], \n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "408d9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "semi_test_X = pickle.load(open(DATA_PATH+'/semi_test_x.pkl','rb'))\n",
    "semi_test_X = {f.name:semi_test_X[f.name] for f in dnn_feature_columns}\n",
    "\n",
    "online_test_loader = preprocess.get_dataloader(semi_test_X, moe, y=None,\n",
    "                                              batch_size=args['batch_size'],\n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "312c44b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = moe.predict(online_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05088022",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv('/home/tione/notebook/wbdc2021/data/wedata/wechat_algo_data2/test_a.csv',\n",
    "                       header=0)\n",
    "df_res = pd.DataFrame(pred_arr)\n",
    "df_res.columns = [\"read_comment\",\"like\",\"click_avatar\",\"forward\",'favorite','comment','follow']\n",
    "\n",
    "test_sub = pd.concat([test_sub, df_res], axis=1)\n",
    "test_sub.loc[test_sub.device==1, 'read_comment'] = 0\n",
    "\n",
    "test_sub[['userid','feedid',\"read_comment\",\"like\",\"click_avatar\",\"forward\",'favorite','comment','follow']]\\\n",
    "    .to_csv('results/semi_nofm.lr0.1.s0.7.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3130b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c29b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc2021_demo",
   "language": "python",
   "name": "conda_wbdc2021_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
