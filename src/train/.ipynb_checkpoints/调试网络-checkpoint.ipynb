{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "597d0da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\n"
     ]
    }
   ],
   "source": [
    "import os, sys, gc, pickle\n",
    "sys.path.append('../')\n",
    "import preprocess\n",
    "from model.moe import MOE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e64a5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "# from deepctr_torch.models.deepfm import *\n",
    "from deepctr_torch.models.basemodel import *\n",
    "from deepctr_torch.models.deepfm import *\n",
    "from deepctr_torch.layers import BiInteractionPooling\n",
    "from deepctr_torch.layers import activation_layer\n",
    "\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from utils import uAUC\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "\n",
    "# TASK_UAUC_WEIGHT = {\n",
    "#     'read_comment':0.4,\n",
    "#     'like': 0.3,\n",
    "#     'click_avatar': 0.2,\n",
    "#     'forward':0.1,\n",
    "#     'favorite': 0.1,\n",
    "#     'comment': 0.1,\n",
    "#     'follow':0.1\n",
    "# }\n",
    "\n",
    "TASK_UAUC_WEIGHT = {\n",
    "    'read_comment':4/13,\n",
    "    'like': 3/13,\n",
    "    'click_avatar': 2/13,\n",
    "    'forward':1/13,\n",
    "    'favorite': 1/13,\n",
    "    'comment': 1/13,\n",
    "    'follow':1/13\n",
    "}\n",
    "\n",
    "class GHM_Loss(nn.Module):\n",
    "    def __init__(self, bins, alpha=0.75, reduction='sum'):\n",
    "        super(GHM_Loss, self).__init__()\n",
    "        self._bins = bins\n",
    "        self._alpha = alpha\n",
    "        self._last_bin_count = None\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def _g2bin(self, g):\n",
    "        return torch.floor(g * (self._bins - 0.0001)).long()\n",
    "\n",
    "    def _custom_loss(self, x, target, weight):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        g = torch.abs(self._custom_loss_grad(x, target)).detach()\n",
    "\n",
    "        bin_idx = self._g2bin(g)\n",
    "\n",
    "        bin_count = torch.zeros((self._bins))\n",
    "        for i in range(self._bins):\n",
    "            bin_count[i] = (bin_idx == i).sum().item()\n",
    "\n",
    "        N = (x.size(0) * x.size(1))\n",
    "\n",
    "        if self._last_bin_count is None:\n",
    "            self._last_bin_count = bin_count\n",
    "        else:\n",
    "            bin_count = self._alpha * self._last_bin_count + (1 - self._alpha) * bin_count\n",
    "            self._last_bin_count = bin_count\n",
    "\n",
    "        nonempty_bins = (bin_count > 0).sum().item()\n",
    "\n",
    "        gd = bin_count * nonempty_bins\n",
    "        gd = torch.clamp(gd, min=0.0001)\n",
    "        beta = N / gd\n",
    "\n",
    "        return self._custom_loss(x, target, beta[bin_idx])\n",
    "\n",
    "\n",
    "class GHMC_Loss(GHM_Loss):\n",
    "    # 分类损失\n",
    "    def __init__(self, bins, alpha):\n",
    "        super(GHMC_Loss, self).__init__(bins, alpha)\n",
    "\n",
    "    def _custom_loss(self, x, target, weight=None):\n",
    "        return F.binary_cross_entropy_with_logits(x, target, weight=weight, reduction=self.reduction)\n",
    "\n",
    "    def _custom_loss_grad(self, x, target):\n",
    "        return torch.sigmoid(x).detach() - target\n",
    "\n",
    "\n",
    "def create_embedding_matrix(feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        # nn.init.kaiming_uniform_(tensor.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "\n",
    "      Arguments\n",
    "        - **inputs_dim**: input feature dimension.\n",
    "\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "\n",
    "        - **activation**: Activation function to use.\n",
    "\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024, device='cpu'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                # nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "                # nn.init.kaiming_uniform_(tensor, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.kaiming_normal_(tensor, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.linears[i](deep_input)\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "            fc = self.dropout(fc)\n",
    "            \n",
    "            deep_input = fc\n",
    "        return deep_input\n",
    "\n",
    "\n",
    "\n",
    "def create_embedding_from_pretrained(feature_columns, init_std=0.0001, linear=False, sparse=False, \n",
    "                                     device='cpu', pretrained_embedding_dict=None, frozen_pretrained=False):\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    # for varlen sparse features, {embedding_name: nn.EmbeddingBag}\n",
    "    sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    varlen_sparse_feature_columns = list(\n",
    "        filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "\n",
    "    embedding_dict = nn.ModuleDict(\n",
    "        {feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "         for feat in\n",
    "         sparse_feature_columns + varlen_sparse_feature_columns}\n",
    "    )\n",
    "\n",
    "    # for feat in varlen_sparse_feature_columns:\n",
    "    #     embedding_dict[feat.embedding_name] = nn.EmbeddingBag(\n",
    "    #         feat.dimension, embedding_size, sparse=sparse, mode=feat.combiner)\n",
    "\n",
    "    for name, tensor in embedding_dict.items():\n",
    "        if (pretrained_embedding_dict is not None) and  (name in pretrained_embedding_dict):\n",
    "            tensor.weight.data.copy_(torch.from_numpy(pretrained_embedding_dict[name]))\n",
    "            if frozen_pretrained:\n",
    "                tensor.weight.requires_grad = False\n",
    "        else:\n",
    "            nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "class MyBaseModel(BaseModel):\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, l2_reg_linear=1e-5, l2_reg_embedding=1e-5,\n",
    "                 init_std=0.0001, seed=1024, task='binary', device='cpu', gpus=None, \n",
    "                 pretrained_embedding_dict=None, frozen_pretrained=False, num_tasks=4):\n",
    "\n",
    "        super(BaseModel, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        \n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        self.device = device\n",
    "        self.gpus = gpus\n",
    "        if gpus and str(self.gpus[0]) not in self.device:\n",
    "            raise ValueError(\n",
    "                \"`gpus[0]` should be the same gpu with `device`\")\n",
    "\n",
    "        self.feature_index = build_input_features(\n",
    "            linear_feature_columns + dnn_feature_columns)\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "\n",
    "        self.embedding_dict = create_embedding_matrix(dnn_feature_columns, init_std, sparse=False, device=device)\n",
    "        # self.embedding_dict = create_embedding_from_pretrained(dnn_feature_columns, init_std, sparse=False, \n",
    "        #                                                       device=device, pretrained_embedding_dict=pretrained_embedding_dict,\n",
    "        #                                                       frozen_pretrained=frozen_pretrained)\n",
    "\n",
    "        self.regularization_weight = []\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "        # parameters of callbacks\n",
    "        self._is_graph_network = True  # used for ModelCheckpoint\n",
    "        self.stop_training = False  # used for EarlyStopping\n",
    "        self.history = History()\n",
    "\n",
    "    def fit(self, train_loader, epochs=1, verbose=1, initial_epoch=0, validation_split=0.,\n",
    "            validation_data=None, val_userid_list=None, shuffle=True, callbacks=None, num_warm_epochs=1, \n",
    "            lr_scheduler=True, scheduler_epochs=5, reduction='sum', task_weight=None, task_dict=None,\n",
    "            num_workers=2, scheduler_method='cos', early_stop_uauc=0.55, label_smoothing=0.2):\n",
    "        \"\"\"\n",
    "        :param x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).If input layers in the model are named, you can also pass a\n",
    "            dictionary mapping input names to Numpy arrays.\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 256.\n",
    "        :param epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.\n",
    "        :param verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        :param initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "        :param validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling.\n",
    "        :param validation_data: tuple `(x_val, y_val)` or tuple `(x_val, y_val, val_sample_weights)` on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`.\n",
    "        :param shuffle: Boolean. Whether to shuffle the order of the batches at the beginning of each epoch.\n",
    "        :param callbacks: List of `deepctr_torch.callbacks.Callback` instances. List of callbacks to apply during training and validation (if ). See [callbacks](https://tensorflow.google.cn/api_docs/python/tf/keras/callbacks). Now available: `EarlyStopping` , `ModelCheckpoint`\n",
    "\n",
    "        :return: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "        \"\"\"\n",
    "        do_validation = False\n",
    "        if validation_data and len(validation_data)==2:\n",
    "            do_validation = True\n",
    "            val_x_loader, val_y = validation_data\n",
    "        else:\n",
    "            val_y = []\n",
    "\n",
    "        model = self.train()\n",
    "        loss_func = self.loss_func\n",
    "        optim = self.optim\n",
    "        if lr_scheduler:\n",
    "            if scheduler_method=='linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                   optim,\n",
    "                   num_warmup_steps=int(len(train_loader)*num_warm_epochs),\n",
    "                   num_training_steps=int(len(train_loader)*(scheduler_epochs)))\n",
    "            else:\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                   optim,\n",
    "                   num_warmup_steps=int(len(train_loader)*num_warm_epochs),\n",
    "                   num_training_steps=int(len(train_loader)*(scheduler_epochs)))\n",
    "        # print('0000 - - reload')\n",
    "        if self.gpus:\n",
    "            print('parallel running on these gpus:', self.gpus)\n",
    "            model = torch.nn.DataParallel(model, device_ids=self.gpus)\n",
    "            batch_size *= len(self.gpus)  # input `batch_size` is batch_size per gpu\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        steps_per_epoch = len(train_loader)\n",
    "\n",
    "        # configure callbacks\n",
    "        callbacks = (callbacks or []) + [self.history]  # add history callback\n",
    "        callbacks = CallbackList(callbacks)\n",
    "        callbacks.on_train_begin()\n",
    "        callbacks.set_model(self)\n",
    "        if not hasattr(callbacks, 'model'):\n",
    "            callbacks.__setattr__('model', self)\n",
    "        callbacks.model.stop_training = False\n",
    "\n",
    "        # Train\n",
    "        best_metric = -1\n",
    "        early_stopping_flag = False\n",
    "        result_logs = {}\n",
    "        logger.info(\"Train on {0} samples, validate on {1} samples, {2} steps per epoch\".format(\n",
    "            len(train_loader)*train_loader.batch_size, len(val_y), steps_per_epoch))\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            callbacks.on_epoch_begin(epoch)\n",
    "            epoch_logs = {}\n",
    "            start_time = time.time()\n",
    "            loss_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            sample_num = 0\n",
    "            train_result = {}\n",
    "            try:\n",
    "                with tqdm(enumerate(train_loader), disable=verbose != 1) as t:\n",
    "                    for _, (x_train, y_train) in t:\n",
    "                        x = x_train.to(self.device).float()\n",
    "                        y = y_train.to(self.device).float()\n",
    "                        sample_num += x.shape[0]\n",
    "                        y_pred = model(x)#.squeeze()\n",
    "\n",
    "                        optim.zero_grad()\n",
    "                        reg_loss = self.get_regularization_loss()\n",
    "                        total_loss = reg_loss + self.aux_loss\n",
    "                        for i in range(self.num_tasks):\n",
    "                            y_task = y[:,i] * (1 - label_smoothing) + 0.5 * label_smoothing\n",
    "                            loss = loss_func(y_pred[:,i], y_task, reduction=reduction)  # y.squeeze()\n",
    "                            total_loss += loss*task_weight[i]\n",
    "                            loss_epoch += loss.item()\n",
    "                            # 输出日志用\n",
    "                            loss_name = task_dict[i]+'_loss'\n",
    "                            if loss_name not in train_result:\n",
    "                                train_result[loss_name] = []\n",
    "                            train_result[loss_name].append(loss.item())\n",
    "                        \n",
    "                        total_loss_epoch += total_loss.item()\n",
    "                        total_loss.backward()\n",
    "                        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                        #torch.nn.utils.clip_grad_value_(model.parameters(), 10.)\n",
    "                        optim.step()\n",
    "                        if lr_scheduler:\n",
    "                            scheduler.step() \n",
    "                        \n",
    "                        # 为节约时间，训练集只输出loss，不做评估\n",
    "                        if False: # verbose>0:\n",
    "                            for name, metric_fun in self.metrics.items():\n",
    "                                # 训练集不做auc和uauc的评估\n",
    "                                if (name=='uauc'): continue\n",
    "                                if (name=='auc'): continue\n",
    "                                # 每个评估函数都要评估多个任务\n",
    "                                for i in range(model.num_tasks):\n",
    "                                    task_name = task_dict[i]+'_'+name\n",
    "                                    if task_name not in train_result:\n",
    "                                        train_result[task_name] = []\n",
    "                                    try:\n",
    "                                        train_result[task_name].append(metric_fun(\n",
    "                                            y[:,i].squeeze().cpu().data.numpy(), \n",
    "                                            y_pred[i].cpu().data.numpy().astype('float64')))\n",
    "                                    except:\n",
    "                                        pass\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                t.close()\n",
    "                raise\n",
    "            t.close()\n",
    "\n",
    "            # Add epoch_logs, training logs\n",
    "            epoch_logs[\"loss\"] = total_loss_epoch / sample_num\n",
    "            for name, result in train_result.items():\n",
    "                epoch_logs[name] = np.sum(result) / sample_num  # len(result)\n",
    "\n",
    "            if do_validation:\n",
    "                eval_result = self.evaluate(val_x_loader, val_y, val_userid_list, task_dict)\n",
    "                for name, result in eval_result.items():\n",
    "                    epoch_logs[\"val_\" + name] = result\n",
    "                    \n",
    "            # verbose\n",
    "            if verbose > 0:\n",
    "                epoch_time = int(time.time() - start_time)\n",
    "                eval_str = 'Epoch {0}/{1}'.format(epoch + 1, epochs)\n",
    "\n",
    "                eval_str += \" {0}s - loss: {1: .4f}\".format(epoch_time, epoch_logs[\"loss\"])\n",
    "                \n",
    "                # 输出训练集的评估结果\n",
    "                # for name in self.metrics:\n",
    "                #     if name=='uauc': continue\n",
    "                #     for i in range(self.num_tasks):\n",
    "                #         task_name = task_dict[i]+'_'+name\n",
    "                #         eval_str += \" - \" + task_name + \": {0: .4f}\".format(epoch_logs[task_name])\n",
    "                for name, result in train_result.items():\n",
    "                    eval_str += \" - \" + name + \": {0: .4f}\".format(epoch_logs[name])\n",
    "                \n",
    "                # 输出验证集的评估结果\n",
    "                if do_validation:\n",
    "                    for name in self.metrics:\n",
    "                        for i in range(self.num_tasks):\n",
    "                            task_name = task_dict[i]+'_'+name\n",
    "                            eval_str += \" - \" + \"val_\" + task_name + \\\n",
    "                                    \": {0: .4f}\".format(epoch_logs[\"val_\" + task_name])\n",
    "\n",
    "                    best_metric = max(best_metric, eval_result['UAUC'])\n",
    "                    if best_metric<=early_stop_uauc:\n",
    "                        # 验证集uauc<=0.55时，提前终止训练，以节约时间\n",
    "                        early_stopping_flag = True\n",
    "                    epoch_logs['val_UAUC'] = eval_result['UAUC']\n",
    "                    eval_str += ' - val_UAUC: {0: .5f}'.format(eval_result['UAUC'])\n",
    "                logger.info(eval_str)\n",
    "                \n",
    "            result_logs['epoch'+str(epoch+1)] = epoch_logs\n",
    "            # 验证集uauc==0.5时，提前终止训练，以节约时间\n",
    "            if early_stopping_flag:\n",
    "                callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "                callbacks.on_train_end()\n",
    "                return self.history, best_metric, result_logs\n",
    "                \n",
    "            callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "            if self.stop_training:\n",
    "                break\n",
    "\n",
    "        callbacks.on_train_end()\n",
    "\n",
    "        return self.history, best_metric, result_logs\n",
    "\n",
    "    def evaluate(self, x, y, userid_list, task_dict=None):\n",
    "        \"\"\"\n",
    "        :param x: Numpy array of test data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs).\n",
    "        :param y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs).\n",
    "        :param batch_size: Integer or `None`. Number of samples per evaluation step. If unspecified, `batch_size` will default to 256.\n",
    "        :return: Dict contains metric names and metric values.\n",
    "        \"\"\"\n",
    "        pred_ans = self.predict(x)\n",
    "        eval_result = {}\n",
    "        eval_result['UAUC'] = 0\n",
    "        \n",
    "        # 外层多个评估函数\n",
    "        for name, metric_fun in self.metrics.items():\n",
    "            # 内层每个评估函数都要对多个任务进行评估\n",
    "            # 生成参数\n",
    "            if name=='uauc':\n",
    "                uauc, task_uaucs = metric_fun(y, pred_ans, userid_list) \n",
    "                eval_result['UAUC'] = uauc\n",
    "                for i in range(self.num_tasks):\n",
    "                    eval_result[task_dict[i]+'_'+name] = task_uaucs[i]\n",
    "            else:\n",
    "                for i in range(self.num_tasks):\n",
    "                    eval_result[task_dict[i]+'_'+name] = metric_fun(y[:,i], pred_ans[:,i])\n",
    "            \n",
    "        return eval_result\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"\n",
    "        :param x: The input data, as a Numpy array (or list of Numpy arrays if the model has multiple inputs).\n",
    "        :param batch_size: Integer. If unspecified, it will default to 256.\n",
    "        :return: Numpy array(s) of predictions.\n",
    "        \"\"\"\n",
    "        model = self.eval()\n",
    "\n",
    "        pred_ans = np.empty([0, 7])\n",
    "        with torch.no_grad():\n",
    "            for _, (x_test) in enumerate(test_loader):\n",
    "                x = x_test[0].to(self.device).float()\n",
    "                y_pred = model(x).cpu().numpy()  # .squeeze()\n",
    "                pred_ans = np.vstack([pred_ans, y_pred])\n",
    "\n",
    "        return pred_ans\n",
    "\n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        # For a Parameter, put it in a list to keep Compatible with get_regularization_loss()\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        # For generators, filters and ParameterLists, convert them to a list of tensors to avoid bugs.\n",
    "        # e.g., we can't pickle generator objects when we save the model.\n",
    "        else:\n",
    "            weight_list = list(weight_list)\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "\n",
    "    def get_regularization_loss(self, ):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight_list, l1, l2 in self.regularization_weight:\n",
    "            for w in weight_list:\n",
    "                if isinstance(w, tuple):\n",
    "                    parameter = w[1]  # named_parameters\n",
    "                else:\n",
    "                    parameter = w\n",
    "                if l1 > 0:\n",
    "                    total_reg_loss += torch.sum(l1 * torch.abs(parameter))\n",
    "                if l2 > 0:\n",
    "                    try:\n",
    "                        total_reg_loss += torch.sum(l2 * torch.square(parameter))\n",
    "                    except AttributeError:\n",
    "                        total_reg_loss += torch.sum(l2 * parameter * parameter)\n",
    "\n",
    "        return total_reg_loss\n",
    "\n",
    "    def add_auxiliary_loss(self, aux_loss, alpha):\n",
    "        self.aux_loss = aux_loss * alpha\n",
    "\n",
    "    def compile(self, optimizer,\n",
    "                learning_rate=0.01,\n",
    "                loss=None,\n",
    "                metrics=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        :param optimizer: String (name of optimizer) or optimizer instance. See [optimizers](https://pytorch.org/docs/stable/optim.html).\n",
    "        :param loss: String (name of objective function) or objective function. See [losses](https://pytorch.org/docs/stable/nn.functional.html#loss-functions).\n",
    "        :param metrics: List of metrics to be evaluated by the model during training and testing. Typically you will use `metrics=['accuracy']`.\n",
    "        \"\"\"\n",
    "        self.metrics_names = [\"loss\"]\n",
    "        self.optim = self._get_optim(optimizer, learning_rate)\n",
    "        self.loss_func = self._get_loss_func(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "\n",
    "    def _get_optim(self, optimizer, learning_rate):\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.parameters(), lr=learning_rate)  # 0.001\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.parameters(), lr=learning_rate)  # 0.01\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.parameters(), lr=learning_rate)\n",
    "            elif optimizer == 'adamw':\n",
    "                optim = torch.optim.AdamW(self.parameters(), lr=learning_rate, weight_decay=0.)\n",
    "            elif optimizer == 'adamax':\n",
    "                optim = torch.optim.Adamax(self.parameters(), lr=learning_rate, weight_decay=0.)\n",
    "            elif optimizer == 'momentum':\n",
    "                optim = torch.optim.SGD(self.parameters(),lr=learning_rate,momentum=0.9,nesterov=True)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            optim = optimizer\n",
    "        return optim\n",
    "    \n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                \n",
    "                # 添加uauc metric\n",
    "                if metric == 'uauc':\n",
    "                    metrics_[metric] = uAUC\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "\n",
    "    def _get_loss_func(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true,\n",
    "                        y_pred,\n",
    "                        eps,\n",
    "                        normalize,\n",
    "                        sample_weight,\n",
    "                        labels)\n",
    "\n",
    "\n",
    "class PretrainedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, init_weight):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(init_weight))\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)\n",
    "\n",
    "\n",
    "class MOE(MyBaseModel):\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, use_fm=True, use_nfm=False,\n",
    "                 dnn_hidden_units=(256, 128), l2_reg_linear=0.00001, l2_reg_embedding=0.00001, \n",
    "                 l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, bi_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=True, task='binary', device='cpu', gpus=None,\n",
    "                 pretrained_embedding_dict=None, frozen_pretrained=False, num_tasks=4, \n",
    "                 pretrained_user_emb_weight=None, pretrained_author_emb_weight=None,\n",
    "                 pretrained_feed_emb_weight=None, pretrained_bgm_song_emb_weight=None,\n",
    "                 pretrained_bgm_singer_emb_weight=None):\n",
    "\n",
    "        super(MOE, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                       l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                       device=device, gpus=gpus, pretrained_embedding_dict=pretrained_embedding_dict, \n",
    "                                       frozen_pretrained=frozen_pretrained, num_tasks=num_tasks)\n",
    "        \n",
    "        dnn_hidden_units = dnn_hidden_units if len(dnn_hidden_units)==num_tasks and isinstance(dnn_hidden_units[0], tuple)\\\n",
    "            else [dnn_hidden_units for i in range(num_tasks)]\n",
    "        self.num_tasks = num_tasks\n",
    "        self.use_fm = use_fm\n",
    "        self.use_nfm = use_nfm\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(dnn_hidden_units) > 0\n",
    "        \n",
    "        self.pretrained_user = pretrained_user_emb_weight is not None\n",
    "        self.pretrained_feed = pretrained_feed_emb_weight is not None\n",
    "        self.pretrained_author = pretrained_author_emb_weight is not None\n",
    "        self.user_emb = []\n",
    "        self.feed_emb = []\n",
    "        self.author_emb = []\n",
    "\n",
    "        self.pretrained_emb_dim = 0\n",
    "        # 载入预训练的的Embedding矩阵\n",
    "        if self.pretrained_user:\n",
    "            for emb_w in pretrained_user_emb_weight:\n",
    "                self.user_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                                         emb_w.shape[1],\n",
    "                                                         emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.user_emb = nn.ModuleList(self.user_emb)\n",
    "        \n",
    "        if self.pretrained_author:\n",
    "            for emb_w in pretrained_author_emb_weight:\n",
    "                self.author_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                                emb_w.shape[1],\n",
    "                                                emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.author_emb = nn.ModuleList(self.author_emb)\n",
    "\n",
    "        if self.pretrained_feed:\n",
    "            for emb_w in pretrained_feed_emb_weight:\n",
    "                self.feed_emb.append(PretrainedEmbedding(emb_w.shape[0],\n",
    "                                            emb_w.shape[1],\n",
    "                                            emb_w).to(device))\n",
    "                self.pretrained_emb_dim += emb_w.shape[1]\n",
    "            self.feed_emb = nn.ModuleList(self.feed_emb)\n",
    "\n",
    "\n",
    "\n",
    "        self.linear_model = nn.ModuleList([Linear(linear_feature_columns, self.feature_index, device=device)\n",
    "                                           for i in range(self.num_tasks)])\n",
    "        for task_linear in self.linear_model:\n",
    "            self.add_regularization_weight(task_linear.parameters(), l2=l2_reg_linear)\n",
    "        \n",
    "        if use_fm:\n",
    "            self.fm = FM()\n",
    "        \n",
    "        dnn_input_dim = self.compute_input_dim(dnn_feature_columns)\n",
    "        dnn_input_dim += self.pretrained_emb_dim\n",
    "\n",
    "        if self.use_dnn:\n",
    "            self.dnn = nn.ModuleList([DNN(dnn_input_dim, dnn_hidden_units[i],\n",
    "                           activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device) for i in range(self.num_tasks)])\n",
    "            self.dnn_linear = nn.ModuleList([nn.Linear(dnn_hidden_units[i][-1], 1, bias=False)\n",
    "                                             for i in range(self.num_tasks)]).to(device)\n",
    "        \n",
    "            for task_dnn in self.dnn:\n",
    "                self.add_regularization_weight(\n",
    "                    filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], task_dnn.named_parameters()), l2=l2_reg_dnn)\n",
    "            for task_out_linear in self.dnn_linear:\n",
    "                self.add_regularization_weight(task_out_linear.weight, l2=l2_reg_dnn)\n",
    "                \n",
    "        self.out = nn.ModuleList([PredictionLayer(task, ) for i in range(self.num_tasks)])\n",
    "        self.to(device)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # pretrained_id => [userid, authorid, feedid]\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        \n",
    "        logit = [linear(X) for linear in self.linear_model]\n",
    "\n",
    "        if self.use_fm and len(sparse_embedding_list) > 0:\n",
    "            fm_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            fm_logit = self.fm(fm_input)\n",
    "            logit = [i+fm_logit for i in logit]\n",
    "        \n",
    "        pretrained_emb = []\n",
    "        # 预训练Embedding\n",
    "        if self.pretrained_user:\n",
    "            for emb in self.user_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['userid'][0]].long()))\n",
    "        if self.pretrained_author:\n",
    "            for emb in self.author_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['authorid'][0]].long()))\n",
    "        if self.pretrained_feed:\n",
    "            for emb in self.feed_emb:\n",
    "                pretrained_emb.append(emb(X[:,self.feature_index['feedid'][0]].long()))\n",
    "\n",
    "\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "        dnn_input = torch.cat([dnn_input]+pretrained_emb, dim=1)\n",
    "\n",
    "        if self.use_dnn:\n",
    "            i = 0\n",
    "            for task_dnn, task_dnn_linear, task_out in zip(self.dnn, self.dnn_linear, self.out):\n",
    "                dnn_output = task_dnn(dnn_input)\n",
    "                dnn_logit = task_dnn_linear(dnn_output)\n",
    "                logit[i]+= dnn_logit\n",
    "                logit[i] = task_out(logit[i])\n",
    "                i += 1\n",
    "        # print(torch.cat(logit, dim=1).shape)\n",
    "        return torch.cat(logit, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da5d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, gc, pickle\n",
    "sys.path.append('../')\n",
    "import preprocess\n",
    "# from model.moe import MOE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1afabfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../my_data/w2v_models_sg_ns_64_epoch30//userid_by_feedid_w10_iter10.64d.pkl\n",
      "classes numbers:  219999\n",
      "word2vec vocab size:  219999\n",
      "Total Random initialized word embedding counts:  0\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//userid_by_authorid_w10_iter10.64d.pkl\n",
      "classes numbers:  219999\n",
      "word2vec vocab size:  219999\n",
      "Total Random initialized word embedding counts:  0\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//authorid_w7_iter10.64d.pkl\n",
      "classes numbers:  18789\n",
      "word2vec vocab size:  18788\n",
      "Total Random initialized word embedding counts:  1\n",
      "../my_data/w2v_models_sg_ns_64_epoch30//feedid_w7_iter10.64d.pkl\n",
      "classes numbers:  106444\n",
      "word2vec vocab size:  103864\n",
      "Total Random initialized word embedding counts:  2580\n",
      "../my_data/official_feed_emb.d512.pkl\n",
      "classes numbers:  106444\n",
      "word2vec vocab size:  106444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 11:05:25 - INFO - __main__ -   All used features:\n",
      "07/27/2021 11:05:25 - INFO - __main__ -   dict_keys(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'videoplayseconds_bin', 'bgm_na', 'feed_machine_tag_tfidf_cls_32', 'feed_machine_kw_tfidf_cls_17', 'author_machine_tag_tfidf_cls_21', 'author_machine_kw_tfidf_cls_18', 'videoplayseconds', 'tag_manu_machine_corr'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Random initialized word embedding counts:  0\n"
     ]
    }
   ],
   "source": [
    "W2V_SG_EPOCH20 = '../my_data/w2v_models_sg_ns_64_epoch20/'\n",
    "W2V_SG_EPOCH30 = '../my_data/w2v_models_sg_ns_64_epoch30/'\n",
    "W2V_SG_EPOCH40 = '../my_data/w2v_models_sg_ns_64_epoch40/'\n",
    "\n",
    "\n",
    "W2V_CBOW_EPOCH20 = '../my_data/w2v_models_cbow_ns_64_epoch20/'\n",
    "W2V_CBOW_EPOCH30 = '../my_data/w2v_models_cbow_ns_64_epoch30/'\n",
    "\n",
    "TOPIC_COLS = ['feed_manu_tag_topic_class', 'feed_machine_tag_topic_class', 'feed_manu_kw_topic_class', \n",
    "              'feed_machine_kw_topic_class', 'feed_description_topic_class', 'author_description_topic_class', \n",
    "              'author_manu_kw_topic_class', 'author_machine_kw_topic_class', 'author_manu_tag_topic_class', \n",
    "              'author_machine_tag_topic_class']\n",
    "\n",
    "pretrained_models = {\n",
    "    'sg_ns_64_epoch20':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_SG_EPOCH20}/feedid_w7_iter10.64d.pkl',\n",
    "        'feed_description_tfidf_cls_18':f'{W2V_SG_EPOCH20}/feed_description_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'feed_machine_kw_tfidf_cls_17':f'{W2V_SG_EPOCH20}/feed_machine_kw_tfidf_cls_17_w7_iter10.64d.pkl',\n",
    "        'feed_machine_tag_tfidf_cls_32':f'{W2V_SG_EPOCH20}/feed_machine_tag_tfidf_cls_32_w7_iter10.64d.pkl',\n",
    "        'feed_manu_kw_tfidf_cls_22':f'{W2V_SG_EPOCH20}/feed_manu_kw_tfidf_cls_22_w7_iter10.64d.pkl',\n",
    "        'feed_manu_tag_tfidf_cls_32':f'{W2V_SG_EPOCH20}/feed_manu_tag_tfidf_cls_32_w7_iter10.64d.pkl',\n",
    "\n",
    "        'authorid': f'{W2V_SG_EPOCH20}/authorid_w7_iter10.64d.pkl',\n",
    "        'author_description_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_description_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_machine_kw_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_machine_kw_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_machine_tag_tfidf_cls_21':f'{W2V_SG_EPOCH20}/author_machine_tag_tfidf_cls_21_w7_iter10.64d.pkl',\n",
    "        'author_manu_kw_tfidf_cls_18':f'{W2V_SG_EPOCH20}/author_manu_kw_tfidf_cls_18_w7_iter10.64d.pkl',\n",
    "        'author_manu_tag_tfidf_cls_19':f'{W2V_SG_EPOCH20}/author_manu_tag_tfidf_cls_19_w7_iter10.64d.pkl',\n",
    "\n",
    "        'userid_by_feed': f'{W2V_SG_EPOCH20}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_SG_EPOCH20}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    },\n",
    "    'sg_ns_64_epoch30':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_SG_EPOCH30}/feedid_w7_iter10.64d.pkl',\n",
    "        'authorid': f'{W2V_SG_EPOCH30}/authorid_w7_iter10.64d.pkl',\n",
    "        'userid_by_feed': f'{W2V_SG_EPOCH30}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_SG_EPOCH30}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    },\n",
    "    'sg_ns_64_epoch40':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_SG_EPOCH40}/feedid_w7_iter10.64d.pkl',\n",
    "        'authorid': f'{W2V_SG_EPOCH40}/authorid_w7_iter10.64d.pkl',\n",
    "        'userid_by_feed': f'{W2V_SG_EPOCH40}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_SG_EPOCH40}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    },\n",
    "    'cbow_ns_64_epoch20':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_CBOW_EPOCH20}/feedid_w7_iter10.64d.pkl',\n",
    "        'authorid': f'{W2V_CBOW_EPOCH20}/authorid_w7_iter10.64d.pkl',\n",
    "        'userid_by_feed': f'{W2V_CBOW_EPOCH20}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_CBOW_EPOCH20}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    },\n",
    "    'cbow_ns_64_epoch30':{\n",
    "        'official_feed': f'../my_data/official_feed_emb.d512.pkl',\n",
    "        'official_feed_pca': f'../my_data/official_feed_emb_pca.d32.pkl',\n",
    "        'feedid': f'{W2V_CBOW_EPOCH30}/feedid_w7_iter10.64d.pkl',\n",
    "        'authorid': f'{W2V_CBOW_EPOCH30}/authorid_w7_iter10.64d.pkl',\n",
    "        'userid_by_feed': f'{W2V_CBOW_EPOCH30}/userid_by_feedid_w10_iter10.64d.pkl',\n",
    "        'userid_by_author': f'{W2V_CBOW_EPOCH30}/userid_by_authorid_w10_iter10.64d.pkl',\n",
    "    }\n",
    "}\n",
    "\n",
    "USED_FEATURES = ['userid','feedid','authorid','bgm_song_id','bgm_singer_id','videoplayseconds_bin','bgm_na',\n",
    "                 'videoplayseconds','tag_manu_machine_corr']+\\\n",
    "                ['feed_machine_tag_tfidf_cls_32','feed_machine_kw_tfidf_cls_17',\n",
    "                 'author_machine_tag_tfidf_cls_21','author_machine_kw_tfidf_cls_18']\n",
    "\n",
    "DATA_PATH = '../my_data/data_base/'\n",
    "\n",
    "args = {}\n",
    "args['USED_FEATURES'] = USED_FEATURES\n",
    "args['DATA_PATH'] = DATA_PATH\n",
    "\n",
    "global hidden_units\n",
    "hidden_units = (512,256,128)\n",
    "args['hidden_units'] = hidden_units\n",
    "args['batch_size'] = 40000\n",
    "args['emb_dim'] = 16\n",
    "args['learning_rate'] = 0.05\n",
    "args['lr_scheduler'] = True\n",
    "args['epochs'] = 2\n",
    "args['scheduler_epochs'] = 3\n",
    "args['num_warm_epochs'] = 0\n",
    "args['scheduler_method'] = 'cos'\n",
    "args['use_bn'] = True\n",
    "args['reduction'] = 'sum'\n",
    "args['optimizer'] = 'adagrad'\n",
    "args['num_tasks'] = 7\n",
    "args['early_stop_uauc'] = 0.689\n",
    "args['num_workers'] = 7\n",
    "args['task_dict'] = {\n",
    "        0: 'read_comment',\n",
    "        1: 'like',\n",
    "        2: 'click_avatar',\n",
    "        3: 'forward',\n",
    "        4: 'favorite',\n",
    "        5: 'comment',\n",
    "        6: 'follow'\n",
    "}\n",
    "args['task_weight'] = {\n",
    "        0: 1,\n",
    "        1: 1,\n",
    "        2: 1,\n",
    "        3: 1,\n",
    "        4: 1,\n",
    "        5: 1,\n",
    "        6: 1\n",
    "}\n",
    "args['opt_iters'] = [10, 10]\n",
    "args['pbounds'] = {'dropout': (0.0, 0.9),\n",
    "                   #'learning_rate': 0.001,\n",
    "                   'l2_reg_dnn': (0.0001,0.0001),\n",
    "                   'l2_reg_embedding': (0.1, 0.1),\n",
    "                   'l2_reg_linear': (0.1, 0.1)\n",
    "                  }\n",
    "\n",
    "args['pretrained_model'] = pretrained_models['sg_ns_64_epoch30']\n",
    "\n",
    "\n",
    "# 全部特征\n",
    "linear_feature_columns = pickle.load(open(DATA_PATH+'/linear_feature.pkl','rb'))\n",
    "dnn_feature_columns = pickle.load(open(DATA_PATH+'/dnn_feature.pkl','rb'))\n",
    "#print('raw:')\n",
    "#print(dnn_feature_columns)\n",
    "# 使用其中部分特征\n",
    "linear_feature_columns = [f for f in linear_feature_columns if f.name in USED_FEATURES]\n",
    "dnn_feature_columns = [f for f in dnn_feature_columns if f.name in USED_FEATURES]\n",
    "features = []\n",
    "for f in linear_feature_columns:\n",
    "    if isinstance(f, SparseFeat):\n",
    "        features.append(SparseFeat(f.name, f.vocabulary_size, args['emb_dim']))\n",
    "    else:\n",
    "        features.append(f)\n",
    "linear_feature_columns = features\n",
    "dnn_feature_columns = features\n",
    "\n",
    "lbe_dict = preprocess.LBE_MODEL\n",
    "\n",
    "pri_train_X = pickle.load(open(DATA_PATH+'/pri_train_x.pkl','rb'))\n",
    "pri_train_y = pickle.load(open(DATA_PATH+'/pri_train_y.pkl','rb'))\n",
    "pri_val_X = pickle.load(open(DATA_PATH+'/pri_val_x.pkl','rb'))\n",
    "pri_val_y = pickle.load(open(DATA_PATH+'/pri_val_y.pkl','rb'))\n",
    "\n",
    "semi_train_X = pickle.load(open(DATA_PATH+'/semi_train_x.pkl','rb'))\n",
    "semi_train_y = pickle.load(open(DATA_PATH+'/semi_train_y.pkl','rb'))\n",
    "semi_val_X = pickle.load(open(DATA_PATH+'/semi_val_x.pkl','rb'))\n",
    "semi_val_y = pickle.load(open(DATA_PATH+'/semi_val_y.pkl','rb'))\n",
    "# 从数据集中选取部分特征\n",
    "semi_train_X = {f.name:semi_train_X[f.name] for f in dnn_feature_columns}\n",
    "semi_val_X = {f.name:semi_val_X[f.name] for f in dnn_feature_columns}\n",
    "pri_train_X = {f.name:pri_train_X[f.name] for f in dnn_feature_columns}\n",
    "pri_val_X = {f.name:pri_val_X[f.name] for f in dnn_feature_columns}\n",
    "\n",
    "# for col in semi_train_X:\n",
    "#     semi_train_X[col] = np.concatenate((semi_train_X[col], pri_train_X[col]), axis=0)\n",
    "# semi_train_y = np.concatenate((semi_train_y, pri_train_y), axis=0)\n",
    "\n",
    "lbe_dict = preprocess.LBE_MODEL\n",
    "# 载入预训练Embedding weight matrix\n",
    "user_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['userid'], \n",
    "                                                    args['pretrained_model']['userid_by_feed'], padding=True)\n",
    "user_by_author_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['userid'], \n",
    "                                                    args['pretrained_model']['userid_by_author'], padding=True)\n",
    "\n",
    "author_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['authorid'], \n",
    "                                                    args['pretrained_model']['authorid'], padding=True)\n",
    "feed_emb_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['feedid'], \n",
    "                                                    args['pretrained_model']['feedid'], padding=True)\n",
    "official_feed_weight = preprocess.load_feature_pretrained_embedding(lbe_dict['feedid'], \n",
    "                                                    args['pretrained_model']['official_feed'], padding=True)\n",
    "\n",
    "logger.info('All used features:')\n",
    "logger.info(semi_train_X.keys())\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e448e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id', 'videoplayseconds_bin', 'bgm_na', 'feed_machine_tag_tfidf_cls_32', 'feed_machine_kw_tfidf_cls_17', 'author_machine_tag_tfidf_cls_21', 'author_machine_kw_tfidf_cls_18', 'videoplayseconds', 'tag_manu_machine_corr'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "_moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=0.,\n",
    "          l2_reg_embedding=0., l2_reg_dnn=0.,\n",
    "          l2_reg_linear=0., device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=None,\n",
    "          pretrained_author_emb_weight=None,\n",
    "          pretrained_feed_emb_weight=None,\n",
    "          )\n",
    "\n",
    "train_loader = preprocess.get_dataloader(semi_train_X, _moe, y=semi_train_y, batch_size=args['batch_size'],  \n",
    "                   num_workers=10)\n",
    "\n",
    "val_loader = preprocess.get_dataloader(semi_val_X, _moe, y=None, batch_size=args['batch_size'],  \n",
    "                   num_workers=10)\n",
    "\n",
    "val_userid_lst = semi_val_X['userid'].tolist()\n",
    "\n",
    "semi_train_X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52bc00f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'target': 0.6984976583814221, 'params': \n",
    "# params = {'dropout': 0.08746740041525639,\n",
    "#           'l2_reg_dnn': 0.0001,\n",
    "#           'l2_reg_embedding': 0.05032424704698356,\n",
    "#           'l2_reg_linear': 0.06932797233659868}\n",
    "\n",
    "# 0.7      |  0.04211  |  0.000423 |  0.06703  |  0.1486\n",
    "# params = {'dropout': 0.04211,\n",
    "#           'l2_reg_dnn': 0.000423,\n",
    "#           'l2_reg_embedding': 0.06703,\n",
    "#           'l2_reg_linear': 0.1486}\n",
    "\n",
    "# 仅复赛数据   线上 0.702389，当前最高单模\n",
    "#'target': 0.6991456900580608,\n",
    "params =  {'dropout': 0.0, 'l2_reg_dnn': 0.001, 'l2_reg_embedding': 0.01, 'l2_reg_linear': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 11:13:51 - INFO - __main__ -   Train on 65880000 samples, validate on 6103955 samples, 1647 steps per epoch\n",
      "60it [00:31,  2.15it/s]"
     ]
    }
   ],
   "source": [
    "# sg 30\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          l2_reg_linear=params['l2_reg_linear'], device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight]\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55, label_smoothing=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f0b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4ed0d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 23:54:38 - INFO - model.moe -   Train on 72480000 samples, validate on 6103955 samples, 1812 steps per epoch\n",
      "1812it [14:35,  2.07it/s]\n",
      "07/24/2021 00:11:29 - INFO - model.moe -   Epoch 1/2 1011s - loss:  0.2665 - read_comment_loss:  0.0908 - like_loss:  0.0909 - click_avatar_loss:  0.0362 - forward_loss:  0.0202 - favorite_loss:  0.0069 - comment_loss:  0.0036 - follow_loss:  0.0054 - val_read_comment_binary_crossentropy:  0.0904 - val_like_binary_crossentropy:  0.0898 - val_click_avatar_binary_crossentropy:  0.0357 - val_forward_binary_crossentropy:  0.0185 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0028 - val_follow_binary_crossentropy:  0.0050 - val_read_comment_auc:  0.9339 - val_like_auc:  0.8573 - val_click_avatar_auc:  0.8760 - val_forward_auc:  0.8955 - val_favorite_auc:  0.9426 - val_comment_auc:  0.9010 - val_follow_auc:  0.8971 - val_read_comment_uauc:  0.6641 - val_like_uauc:  0.6602 - val_click_avatar_uauc:  0.7519 - val_forward_uauc:  0.7414 - val_favorite_uauc:  0.7684 - val_comment_uauc:  0.6278 - val_follow_uauc:  0.7395 - val_UAUC:  0.69369\n",
      "1812it [13:35,  2.22it/s]\n",
      "07/24/2021 00:27:21 - INFO - model.moe -   Epoch 2/2 952s - loss:  0.2431 - read_comment_loss:  0.0851 - like_loss:  0.0862 - click_avatar_loss:  0.0331 - forward_loss:  0.0178 - favorite_loss:  0.0054 - comment_loss:  0.0025 - follow_loss:  0.0043 - val_read_comment_binary_crossentropy:  0.0890 - val_like_binary_crossentropy:  0.0889 - val_click_avatar_binary_crossentropy:  0.0353 - val_forward_binary_crossentropy:  0.0182 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0028 - val_follow_binary_crossentropy:  0.0049 - val_read_comment_auc:  0.9375 - val_like_auc:  0.8622 - val_click_avatar_auc:  0.8804 - val_forward_auc:  0.9042 - val_favorite_auc:  0.9427 - val_comment_auc:  0.9093 - val_follow_auc:  0.9103 - val_read_comment_uauc:  0.6703 - val_like_uauc:  0.6640 - val_click_avatar_uauc:  0.7560 - val_forward_uauc:  0.7464 - val_favorite_uauc:  0.7714 - val_comment_uauc:  0.6391 - val_follow_uauc:  0.7487 - val_UAUC:  0.69929\n"
     ]
    }
   ],
   "source": [
    "# epoch30 \n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          l2_reg_linear=params['l2_reg_linear'], device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight],\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=0.05, loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc']) #args['learning_rate']\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d39adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/23/2021 16:17:07 - INFO - model.moe -   Train on 72480000 samples, validate on 6103955 samples, 1812 steps per epoch\n",
      "34it [00:21,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1812it [14:33,  2.08it/s]\n",
      "07/23/2021 16:33:51 - INFO - model.moe -   Epoch 1/2 1003s - loss:  0.2669 - read_comment_loss:  0.0909 - like_loss:  0.0909 - click_avatar_loss:  0.0362 - forward_loss:  0.0202 - favorite_loss:  0.0071 - comment_loss:  0.0036 - follow_loss:  0.0054 - val_read_comment_binary_crossentropy:  0.0904 - val_like_binary_crossentropy:  0.0897 - val_click_avatar_binary_crossentropy:  0.0357 - val_forward_binary_crossentropy:  0.0185 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0028 - val_follow_binary_crossentropy:  0.0050 - val_read_comment_auc:  0.9339 - val_like_auc:  0.8578 - val_click_avatar_auc:  0.8756 - val_forward_auc:  0.8955 - val_favorite_auc:  0.9417 - val_comment_auc:  0.9052 - val_follow_auc:  0.8980 - val_read_comment_uauc:  0.6633 - val_like_uauc:  0.6602 - val_click_avatar_uauc:  0.7514 - val_forward_uauc:  0.7425 - val_favorite_uauc:  0.7686 - val_comment_uauc:  0.6342 - val_follow_uauc:  0.7381 - val_UAUC:  0.69384\n",
      "1812it [13:31,  2.23it/s]\n",
      "07/23/2021 16:49:31 - INFO - model.moe -   Epoch 2/2 940s - loss:  0.2431 - read_comment_loss:  0.0851 - like_loss:  0.0860 - click_avatar_loss:  0.0331 - forward_loss:  0.0178 - favorite_loss:  0.0054 - comment_loss:  0.0025 - follow_loss:  0.0043 - val_read_comment_binary_crossentropy:  0.0891 - val_like_binary_crossentropy:  0.0888 - val_click_avatar_binary_crossentropy:  0.0353 - val_forward_binary_crossentropy:  0.0182 - val_favorite_binary_crossentropy:  0.0064 - val_comment_binary_crossentropy:  0.0027 - val_follow_binary_crossentropy:  0.0049 - val_read_comment_auc:  0.9374 - val_like_auc:  0.8626 - val_click_avatar_auc:  0.8802 - val_forward_auc:  0.9048 - val_favorite_auc:  0.9420 - val_comment_auc:  0.9132 - val_follow_auc:  0.9102 - val_read_comment_uauc:  0.6686 - val_like_uauc:  0.6635 - val_click_avatar_uauc:  0.7558 - val_forward_uauc:  0.7476 - val_favorite_uauc:  0.7729 - val_comment_uauc:  0.6421 - val_follow_uauc:  0.7480 - val_UAUC:  0.69900\n"
     ]
    }
   ],
   "source": [
    "# epoch20 + user_by_feed embedding\n",
    "\n",
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          l2_reg_linear=params['l2_reg_linear'], device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight],\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=args['learning_rate'], loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc'])\n",
    "\n",
    "metric = moe.fit(train_loader, validation_data=[val_loader, semi_val_y],\n",
    "                   epochs=2, val_userid_list=val_userid_lst,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'],\n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa8cb90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=0.,\n",
    "          l2_reg_embedding=0., l2_reg_dnn=0.,\n",
    "          l2_reg_linear=0., device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=None,\n",
    "          pretrained_author_emb_weight=None,\n",
    "          pretrained_feed_emb_weight=None,\n",
    "          )\n",
    "\n",
    "# 用于线上预测的训练集\n",
    "online_train_X = {}\n",
    "for col in semi_train_X:\n",
    "    online_train_X[col] = np.concatenate((semi_train_X[col], semi_val_X[col], pri_val_X[col]), axis=0)\n",
    "online_train_y = np.concatenate((semi_train_y, semi_val_y, pri_val_y), axis=0)\n",
    "\n",
    "online_train_loader = preprocess.get_dataloader(online_train_X, _moe, y=online_train_y, \n",
    "                                              batch_size=args['batch_size'],  \n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14087801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/24/2021 19:46:05 - INFO - model.moe -   Train on 79200000 samples, validate on 0 samples, 1980 steps per epoch\n",
      "1980it [16:03,  2.05it/s]\n",
      "07/24/2021 20:02:19 - INFO - model.moe -   Epoch 1/2 974s - loss:  0.2661 - read_comment_loss:  0.0907 - like_loss:  0.0905 - click_avatar_loss:  0.0362 - forward_loss:  0.0199 - favorite_loss:  0.0069 - comment_loss:  0.0035 - follow_loss:  0.0054\n",
      "1980it [16:04,  2.05it/s]\n",
      "07/24/2021 20:18:35 - INFO - model.moe -   Epoch 2/2 975s - loss:  0.2426 - read_comment_loss:  0.0847 - like_loss:  0.0858 - click_avatar_loss:  0.0331 - forward_loss:  0.0178 - favorite_loss:  0.0055 - comment_loss:  0.0025 - follow_loss:  0.0044\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2345)\n",
    "import random\n",
    "random.seed(2345)\n",
    "\n",
    "moe = MOE(dnn_hidden_units=args['hidden_units'], linear_feature_columns=linear_feature_columns,\n",
    "          dnn_feature_columns=dnn_feature_columns, task='binary', dnn_dropout=params['dropout'],\n",
    "          l2_reg_embedding=params['l2_reg_embedding'], l2_reg_dnn=params['l2_reg_dnn'],\n",
    "          l2_reg_linear=params['l2_reg_linear'], device=device, seed=1233, num_tasks=args['num_tasks'],\n",
    "          pretrained_user_emb_weight=[user_emb_weight],\n",
    "          pretrained_author_emb_weight=[author_emb_weight],\n",
    "          pretrained_feed_emb_weight=[feed_emb_weight,official_feed_weight],\n",
    "          )\n",
    "\n",
    "moe.compile(optimizer=args['optimizer'], learning_rate=args['learning_rate'], loss=\"binary_crossentropy\", \n",
    "              metrics=[\"binary_crossentropy\",'auc','uauc'])\n",
    "\n",
    "metric = moe.fit(online_train_loader, validation_data=None,\n",
    "                   epochs=2, val_userid_list=None,\n",
    "                   lr_scheduler=args['lr_scheduler'], scheduler_epochs=args['scheduler_epochs'],\n",
    "                   scheduler_method=args['scheduler_method'], num_warm_epochs=args['num_warm_epochs'],\n",
    "                   reduction=args['reduction'], \n",
    "                   task_dict=args['task_dict'], task_weight=args['task_weight'],verbose=1,\n",
    "                   early_stop_uauc=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f41c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "semi_test_X = pickle.load(open(DATA_PATH+'/semi_test_x.pkl','rb'))\n",
    "semi_test_X = {f.name:semi_test_X[f.name] for f in dnn_feature_columns}\n",
    "\n",
    "online_test_loader = preprocess.get_dataloader(semi_test_X, moe, y=None,\n",
    "                                              batch_size=args['batch_size'],\n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac858a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_arr = moe.predict(online_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6efbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv('/home/tione/notebook/wbdc2021/data/wedata/wechat_algo_data2/test_a.csv',\n",
    "                       header=0)\n",
    "df_res = pd.DataFrame(pred_arr)\n",
    "df_res.columns = [\"read_comment\",\"like\",\"click_avatar\",\"forward\",'favorite','comment','follow']\n",
    "\n",
    "test_sub = pd.concat([test_sub, df_res], axis=1)\n",
    "test_sub.loc[test_sub.device==1, 'read_comment'] = 0\n",
    "\n",
    "test_sub[['userid','feedid',\"read_comment\",\"like\",\"click_avatar\",\"forward\",'favorite','comment','follow']]\\\n",
    "    .to_csv('results/pri_semi_all.lr0.05.s0.7.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be8783b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88f99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc2021_demo",
   "language": "python",
   "name": "conda_wbdc2021_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
