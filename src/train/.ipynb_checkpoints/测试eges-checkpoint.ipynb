{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c57395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, gc, sys\n",
    "import pickle\n",
    "sys.path.append('../')\n",
    "from utils import reduce_mem_usage, dict2model\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5c939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d64d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEED_PATH = '../my_data/feedid_text_features/feed_author_text_features_fillna_by_author_clusters.pkl'\n",
    "PAIR_PATH1 = '../my_data/eges/feed_label_eges1_raw.pkl'\n",
    "PAIR_PATH2 = '../my_data/eges/feed_label_eges2_raw.pkl'\n",
    "PAIR_PATH3 = '../my_data/eges/feed_label_eges3_raw.pkl'\n",
    "PAIR_PATH4 = '../my_data/eges/feed_label_eges4_raw.pkl'\n",
    "\n",
    "LBE_PATH = '../my_data/eges/feed_lbe_dict.pkl'\n",
    "\n",
    "USED_COLS = ['feedid','authorid']+['feed_machine_tag_tfidf_cls_32','feed_machine_kw_tfidf_cls_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff13fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~> Memory usage of dataframe is 4.061 MG\n",
      "~> Memory usage after optimization is: 1.624 MG\n",
      "~> Decreased by 60.0%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbe_dict = pickle.load(open(LBE_PATH, 'rb'))\n",
    "feed = pd.read_pickle(FEED_PATH)[USED_COLS].astype(str)\n",
    "\n",
    "for c in lbe_dict:\n",
    "    feed[c] = lbe_dict[c].transform(feed[c])+1\n",
    "\n",
    "feed = reduce_mem_usage(feed)\n",
    "\n",
    "df_pair1 = pd.read_pickle(PAIR_PATH1)\n",
    "df_pair2 = pd.read_pickle(PAIR_PATH2)\n",
    "df_pair3 = pd.read_pickle(PAIR_PATH3)\n",
    "df_pair4 = pd.read_pickle(PAIR_PATH4)\n",
    "\n",
    "df_pair = df_pair1.append(df_pair2).append(df_pair3).append(df_pair4)\n",
    "\n",
    "del df_pair1, df_pair2, df_pair3, df_pair4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85ca341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedid</th>\n",
       "      <th>label</th>\n",
       "      <th>authorid</th>\n",
       "      <th>feed_machine_tag_tfidf_cls_32</th>\n",
       "      <th>feed_machine_kw_tfidf_cls_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105477</td>\n",
       "      <td>26368</td>\n",
       "      <td>13055</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105477</td>\n",
       "      <td>14630</td>\n",
       "      <td>13055</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feedid  label  authorid  feed_machine_tag_tfidf_cls_32  \\\n",
       "0  105477  26368     13055                             31   \n",
       "1  105477  14630     13055                             31   \n",
       "\n",
       "   feed_machine_kw_tfidf_cls_17  \n",
       "0                             6  \n",
       "1                             6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pair = df_pair.merge(feed, on='feedid', how='left')\n",
    "df_pair.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4cb5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair.to_pickle('../my_data/eges/feed_pairs_eges_raw.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6871e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffbd90a8",
   "metadata": {},
   "source": [
    "### 计算各个feed出现频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebe70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_PRI = '../../wbdc2021/data/wedata/wechat_algo_data1/'\n",
    "DATA_PATH_SEMI = '../../wbdc2021/data/wedata/wechat_algo_data2/'\n",
    "\n",
    "user_act_pri = pd.read_csv(f'{DATA_PATH_PRI}/user_action.csv', header=0)[['userid','feedid','date_']]\n",
    "test_a_pri = pd.read_csv(f'{DATA_PATH_PRI}/test_a.csv', header=0)[['userid','feedid']]\n",
    "test_b_pri = pd.read_csv(f'{DATA_PATH_PRI}/test_b.csv', header=0)[['userid','feedid']]\n",
    "test_a_pri['date_'] = 15\n",
    "test_b_pri['date_'] = 15\n",
    "\n",
    "user_act_semi = pd.read_csv(f'{DATA_PATH_SEMI}/user_action.csv', header=0)[['userid','feedid','date_']]\n",
    "test_a_semi = pd.read_csv(f'{DATA_PATH_SEMI}/test_a.csv', header=0)[['userid','feedid']]\n",
    "test_a_semi['date_'] = 15\n",
    "\n",
    "user_act = user_act_semi.append(user_act_pri)\\\n",
    "            .append(test_a_pri)\\\n",
    "            .append(test_b_pri)\\\n",
    "            .append(test_a_semi)\\\n",
    "            .sort_values(by=['userid','date_'])\n",
    "\n",
    "del user_act_pri, user_act_semi, test_a_pri, test_b_pri, test_a_semi\n",
    "gc.collect()\n",
    "\n",
    "user_act = user_act[['userid', 'feedid']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e4b9f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85587121, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total count\n",
    "user_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b02ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_act['feedid'] = lbe_dict['feedid'].transform(user_act['feedid'])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4d8627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各个单词出现频率分布\n",
    "words_count = Counter(user_act['feedid'].tolist())\n",
    "\n",
    "# 根据word index排好序\n",
    "words_count = sorted(words_count.items(), key=lambda x:x[0], reverse=False)\n",
    "counts = np.array([i[1] for i in words_count])\n",
    "# noise_dist = torch.from_numpy(counts**(0.75)/np.sum(counts**(0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44e21150",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(counts, open('../my_data/eges/feed_raw_counts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5fe590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 64, 620, 283,  56,  99,  58, 396, 120, 484,  35])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729ccd48",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da567d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(sparse_columns, varlen_sparse_columns, embed_dim,\n",
    "                            init_std=0.0001, padding=True, device='cpu', mode='mean'):\n",
    "    # sparse_columns => dict{'name':vocab_size}\n",
    "    # Return nn.ModuleDict: for sparse features, {embedding_name: nn.Embedding}\n",
    "    padding_idx = 0 if padding else None\n",
    "    sparse_embedding_dict = {\n",
    "        feat: nn.Embedding(sparse_columns[feat], embed_dim, padding_idx=padding_idx)\n",
    "                             for feat in sparse_columns\n",
    "    }\n",
    "\n",
    "    if varlen_sparse_columns:\n",
    "        varlen_sparse_embedding_dict = {\n",
    "            feat:nn.EmbeddingBag(varlen_sparse_columns[feat], embed_dim, padding_idx=padding_idx,\n",
    "                                 mode=mode) for feat in varlen_sparse_columns\n",
    "        }\n",
    "        sparse_embedding_dict.update(varlen_sparse_embedding_dict)\n",
    "        \n",
    "    embedding_dict = nn.ModuleDict(sparse_embedding_dict)\n",
    "    \n",
    "    for tensor in embedding_dict.values():\n",
    "        nn.init.normal_(tensor.weight, mean=0, std=init_std)\n",
    "        # nn.init.kaiming_uniform_(tensor.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "    return embedding_dict.to(device)\n",
    "\n",
    "\n",
    "class EGES(nn.Module):\n",
    "    def __init__(self, sparse_dict, varlen_sparse_dict=None, target_col='sku_id',\n",
    "                 n_embed=64, k_side=3, noise_dist=None, device='cpu', padding=True):\n",
    "        \"\"\"sparse_dict: dict, {feature_name: vocab_size}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.k_side = k_side\n",
    "        self.device = device\n",
    "        self.padding = padding\n",
    "        self.target_col = target_col\n",
    "        self.features = list(sparse_dict.keys())\n",
    "        if varlen_sparse_dict:\n",
    "            self.features = self.features + list(varlen_sparse_dict.keys())\n",
    "        # 如果padding了的话，则负采样出来的index均需要+1\n",
    "        self.sample_word_offset = 1 if padding else 0\n",
    "        # input embedding dict, include item and side info\n",
    "        self.input_embedding_dict = create_embedding_matrix(\n",
    "            sparse_dict, varlen_sparse_dict, n_embed,\n",
    "            init_std=0.0001, padding=padding, device=device, mode='mean')\n",
    "        self.out_embed = nn.Embedding(sparse_dict[target_col], n_embed,\n",
    "                                      padding_idx=0 if padding else None)\n",
    "        self.attn_embed = nn.Embedding(sparse_dict[target_col], k_side+1, \n",
    "                                       padding_idx=0 if padding else None)\n",
    "        \n",
    "        # Initialize out embedding tables with uniform distribution\n",
    "        nn.init.normal_(self.out_embed.weight, mean=0, std=0.0001)\n",
    "        nn.init.normal_(self.attn_embed.weight, mean=0, std=0.0001)\n",
    "\n",
    "        if noise_dist is None:\n",
    "            # sampling words uniformly\n",
    "            self.noise_dist = torch.ones(sparse_dict[target_col])\n",
    "        else:\n",
    "            self.noise_dist = noise_dist\n",
    "        self.noise_dist = self.noise_dist.to(device)\n",
    "\n",
    "    def forward_input(self, input_dict):\n",
    "        # return input vector embeddings\n",
    "        # version 0.1  average all field embeddings as input vector embeddings\n",
    "        # version 0.2 weighted average all field embeddings as input vector embeddings\n",
    "        embed_lst = []\n",
    "        for col in self.features:\n",
    "            if col in input_dict:\n",
    "                input_vector = self.input_embedding_dict[col](input_dict[col])\n",
    "                embed_lst.append(input_vector)\n",
    "\n",
    "        batch_size = input_vector.shape[0]\n",
    "        # embeds => [batch_size, k_side+1, n_embed]\n",
    "        embeds = torch.cat(embed_lst, dim=1).reshape(batch_size, self.k_side+1, self.n_embed)\n",
    "        \n",
    "        # attation => [batch_size, k_side+1]\n",
    "        attn_w = self.attn_embed(input_dict[self.target_col])\n",
    "        attn_w = torch.exp(attn_w)\n",
    "        attn_s = torch.sum(attn_w, dim=1).reshape(-1, 1)\n",
    "        attn_w = (attn_w/attn_s).reshape(batch_size, 1, self.k_side+1) # 归一化\n",
    "        \n",
    "        # attw => [batch_size, 1, k_side+1]\n",
    "        # embeds => [batch_size, k_side+1, embed_size]\n",
    "        # matmul out => [batch_size, 1, embed_size]\n",
    "        input_vector = torch.matmul(attn_w, embeds).squeeze(1)\n",
    "        \n",
    "        return input_vector\n",
    "\n",
    "    def forward_output(self, output_words):\n",
    "        # return output vector embeddings\n",
    "        output_vector = self.out_embed(output_words)\n",
    "        return output_vector\n",
    "    \n",
    "    def forward_noise(self, batch_size, n_samples):\n",
    "        \"\"\"Generate noise vectors with shape [batch_size, n_samples, n_embed]\n",
    "        \"\"\"\n",
    "        # sample words from our noise distribution\n",
    "        noise_words = torch.multinomial(self.noise_dist, batch_size*n_samples, \n",
    "                                        replacement=True) + self.sample_word_offset\n",
    "        noise_vector = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
    "        \n",
    "        return noise_vector\n",
    "    \n",
    "    def predict_cold_start(self, input_dict):\n",
    "        \"\"\"when predict for cold start item, average all side info instead of attention weight.\n",
    "        \"\"\"\n",
    "        embed_lst = []\n",
    "        for col in self.features:\n",
    "            if (col in input_dict) and (col!=self.target_col):\n",
    "                input_vector = self.input_embedding_dict[col](input_dict[col])\n",
    "                embed_lst.append(input_vector)\n",
    "\n",
    "        batch_size = input_vector.shape[0]\n",
    "        # embeds => [batch_size, k_side+1, n_embed]\n",
    "        embeds = torch.cat(embed_lst, dim=1).reshape(batch_size, self.k_side, self.n_embed)\n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        \n",
    "        return embeds\n",
    "    \n",
    "    def predict(self, input_dict, cold=False):\n",
    "        if cold:\n",
    "            return self.predict_cold_start(input_dict)\n",
    "        \n",
    "        return self.forward_input(input_dict)\n",
    "        \n",
    "\n",
    "\n",
    "class NegativeSamplingLoss(nn.Module):\n",
    "    \"\"\"这里用的是负对数似然, 而不是sampled softmax\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
    "        batch_size, embed_size = input_vectors.shape\n",
    "        \n",
    "        # input vectors should be a batch of column vectors\n",
    "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
    "        \n",
    "        # output vectors should be a batch of row vectors\n",
    "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
    "        \n",
    "        # bmm = batch matrix multiplication\n",
    "        # target words log-sigmoid loss\n",
    "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
    "        \n",
    "        # negative sampling words log-sigmoid loss\n",
    "        # negative words sigmoid optmize to small, thus here noise_vectors.neg()\n",
    "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
    "        # sum the losses over the sample of noise vectors\n",
    "        noise_loss = noise_loss.squeeze().sum(1)\n",
    "        \n",
    "        # sum target and negative loss\n",
    "        return -(out_loss + noise_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b85ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextData(Dataset):\n",
    "    def __init__(self, df, sparse_columns=['feedid','label','authorid','feed_machine_tag_tfidf_cls_32',\n",
    "                                           'feed_machine_kw_tfidf_cls_17'],\n",
    "                 varlen_sparse_columns=[], device='cpu'):\n",
    "        self.sparse_columns = sparse_columns\n",
    "        self.varlen_sparse_columns = varlen_sparse_columns\n",
    "        self.device = device\n",
    "        self.data = {\n",
    "            col:df[col].values for col in sparse_columns\n",
    "        }\n",
    "        if varlen_sparse_columns:\n",
    "            for col in varlen_sparse_columns:\n",
    "                self.data[col] = np.vstack(df[col].values)\n",
    "\n",
    "        self.data_num = len(df)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data_num\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_dic = {}\n",
    "        for col in self.sparse_columns:\n",
    "            data_dic[col] = torch.tensor(self.data[col][idx]).long() #.to(self.device)\n",
    "        if self.varlen_sparse_columns:\n",
    "            for col in self.varlen_sparse_columns:\n",
    "                data_dic[col] = torch.tensor(self.data[col][idx, :]).long() #.to(self.device)\n",
    "\n",
    "        return data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37d08d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feedid': 106445,\n",
       " 'authorid': 18790,\n",
       " 'feed_machine_tag_tfidf_cls_32': 34,\n",
       " 'feed_machine_kw_tfidf_cls_17': 19}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 各个field的维度，包含padding index\n",
    "vocab_dict = {feat:len(lbe_dict[feat].classes_)+1 for feat in lbe_dict}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3afdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算各个单词出现频率分布\n",
    "words_count = Counter(df_pair['feedid'].tolist())\n",
    "tot_count = len(df_pair)\n",
    "\n",
    "# 根据word index排好序\n",
    "words_count = sorted(words_count.items(), key=lambda x:x[0], reverse=False)\n",
    "counts = np.array([i[1] for i in words_count])\n",
    "noise_dist = torch.from_numpy(counts**(0.75)/np.sum(counts**(0.75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7c88b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "textdata = TextData(df_pair, sparse_columns=['feedid','label','authorid','feed_machine_tag_tfidf_cls_32',\n",
    "                                           'feed_machine_kw_tfidf_cls_17']) \n",
    "textloader = DataLoader(textdata, \n",
    "                        batch_size=10000, \n",
    "                        shuffle=True, \n",
    "                        num_workers=10, \n",
    "                        drop_last=False, \n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42ebc760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90881"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "model = EGES(vocab_dict, n_embed=embedding_dim, k_side=3, target_col='feedid',\n",
    "             noise_dist=noise_dist, device=device, padding=True).to(device)\n",
    "criterion = NegativeSamplingLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "for e in range(5):\n",
    "    for i, data_dic in tqdm(enumerate(textloader)):\n",
    "        # input, output and noise vectors\n",
    "        data_dic = {feat:data_dic[feat].to(device) for feat in data_dic}\n",
    "        input_vectors = model.forward_input(data_dic)\n",
    "        output_vectors = model.forward_output(data_dic['label'])\n",
    "        noise_vectors = model.forward_noise(data_dic['label'].shape[0], 10)\n",
    "        # negative sampling loss\n",
    "        loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10000==0:\n",
    "            print(f'Epoch {e}/5  Loss = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6081e",
   "metadata": {},
   "source": [
    "### 预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d04c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEED_PATH = '../my_data/feedid_text_features/feed_author_text_features_fillna_by_author_clusters.pkl'\n",
    "LBE_PATH = '../my_data/eges/feed_lbe_dict.pkl'\n",
    "MODEL1_PATH = '../my_data/eges/feed_raw_model_epoch0.bin'\n",
    "MODEL2_PATH = '../my_data/eges/feed_raw_model_epoch1.bin'\n",
    "MODEL3_PATH = '../my_data/eges/feed_raw_model_epoch2.bin'\n",
    "\n",
    "\n",
    "USED_COLS = ['feedid','authorid']+['feed_machine_tag_tfidf_cls_32','feed_machine_kw_tfidf_cls_17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b39eef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~> Memory usage of dataframe is 4.061 MG\n",
      "~> Memory usage after optimization is: 1.624 MG\n",
      "~> Decreased by 60.0%\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lbe_dict = pickle.load(open(LBE_PATH, 'rb'))\n",
    "feed = pd.read_pickle(FEED_PATH)[USED_COLS].astype(str)\n",
    "\n",
    "for c in lbe_dict:\n",
    "    feed[c] = lbe_dict[c].transform(feed[c])+1\n",
    "\n",
    "feed = reduce_mem_usage(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b4726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdata = TextData(feed, sparse_columns=['feedid','authorid','feed_machine_tag_tfidf_cls_32',\n",
    "                                           'feed_machine_kw_tfidf_cls_17']) \n",
    "textloader = DataLoader(textdata, \n",
    "                        batch_size=10000,\n",
    "                        shuffle=False,\n",
    "                        num_workers=10,\n",
    "                        drop_last=False,\n",
    "                        pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a605c9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu'\n",
    "if device=='gpu' and torch.cuda.is_available():\n",
    "    # print('cuda ready...')\n",
    "    device = 'cuda:1'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "# 各个field的维度，包含padding index\n",
    "vocab_dict = {feat:len(lbe_dict[feat].classes_)+1 for feat in lbe_dict}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df0d52df",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../my_data/eges/feed_raw_model_epoch3.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-37eeec5a9c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m              noise_dist=None, device=device, padding=True).to(device)\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL4_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/notebook/envs/wbdc2021_demo/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../my_data/eges/feed_raw_model_epoch3.bin'"
     ]
    }
   ],
   "source": [
    "feeds = []\n",
    "vectors = []\n",
    "\n",
    "embedding_dim = 64\n",
    "model = EGES(vocab_dict, n_embed=embedding_dim, k_side=3, target_col='feedid',\n",
    "             noise_dist=None, device=device, padding=True).to(device)\n",
    "\n",
    "state_dict = torch.load(MODEL2_PATH, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "for i, data_dic in tqdm(enumerate(textloader)):\n",
    "    # input, output and noise vectors\n",
    "    data_dic = {feat:data_dic[feat].to(device) for feat in data_dic}\n",
    "    input_vectors = model.forward_input(data_dic)\n",
    "    feeds.extend(data_dic['feedid'].tolist())\n",
    "    vectors.append(input_vectors.cpu().data.numpy())\n",
    "    \n",
    "vectors = np.vstack(vectors)\n",
    "feeds_offset = [i-1 for i in feeds]\n",
    "feeds_raw = lbe_dict['feedid'].inverse_transform(feeds_offset)\n",
    "\n",
    "df_emb = pd.DataFrame(vectors)\n",
    "df_emb.columns = [f'eges1_{i}' for i in range(64)]\n",
    "df_emb['feedid'] = feeds_raw\n",
    "\n",
    "mm = dict2model(df_emb, 'feedid', [f'eges1_{i}' for i in range(64)], \n",
    "                save_name='../my_data/eges/feedid_eges1_emb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce1b226b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "636e0d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e116a851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eges0_0</th>\n",
       "      <th>eges0_1</th>\n",
       "      <th>eges0_2</th>\n",
       "      <th>eges0_3</th>\n",
       "      <th>eges0_4</th>\n",
       "      <th>eges0_5</th>\n",
       "      <th>eges0_6</th>\n",
       "      <th>eges0_7</th>\n",
       "      <th>eges0_8</th>\n",
       "      <th>eges0_9</th>\n",
       "      <th>...</th>\n",
       "      <th>eges0_55</th>\n",
       "      <th>eges0_56</th>\n",
       "      <th>eges0_57</th>\n",
       "      <th>eges0_58</th>\n",
       "      <th>eges0_59</th>\n",
       "      <th>eges0_60</th>\n",
       "      <th>eges0_61</th>\n",
       "      <th>eges0_62</th>\n",
       "      <th>eges0_63</th>\n",
       "      <th>feedid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.013009</td>\n",
       "      <td>-0.019129</td>\n",
       "      <td>-0.059938</td>\n",
       "      <td>-0.066357</td>\n",
       "      <td>0.049701</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0.035243</td>\n",
       "      <td>0.098974</td>\n",
       "      <td>0.054288</td>\n",
       "      <td>-0.062221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077801</td>\n",
       "      <td>-0.020655</td>\n",
       "      <td>0.069123</td>\n",
       "      <td>-0.063572</td>\n",
       "      <td>-0.111397</td>\n",
       "      <td>0.092649</td>\n",
       "      <td>0.094472</td>\n",
       "      <td>0.042676</td>\n",
       "      <td>0.071391</td>\n",
       "      <td>43549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.157314</td>\n",
       "      <td>-0.017113</td>\n",
       "      <td>-0.117849</td>\n",
       "      <td>-0.183933</td>\n",
       "      <td>0.129881</td>\n",
       "      <td>0.104452</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>0.096207</td>\n",
       "      <td>-0.034852</td>\n",
       "      <td>-0.065591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183422</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.072671</td>\n",
       "      <td>-0.112357</td>\n",
       "      <td>-0.115335</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.114284</td>\n",
       "      <td>0.120367</td>\n",
       "      <td>-0.037177</td>\n",
       "      <td>77432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    eges0_0   eges0_1   eges0_2   eges0_3   eges0_4   eges0_5   eges0_6  \\\n",
       "0 -0.013009 -0.019129 -0.059938 -0.066357  0.049701  0.024666  0.035243   \n",
       "1 -0.157314 -0.017113 -0.117849 -0.183933  0.129881  0.104452  0.029351   \n",
       "\n",
       "    eges0_7   eges0_8   eges0_9  ...  eges0_55  eges0_56  eges0_57  eges0_58  \\\n",
       "0  0.098974  0.054288 -0.062221  ...  0.077801 -0.020655  0.069123 -0.063572   \n",
       "1  0.096207 -0.034852 -0.065591  ...  0.183422  0.038050  0.072671 -0.112357   \n",
       "\n",
       "   eges0_59  eges0_60  eges0_61  eges0_62  eges0_63  feedid  \n",
       "0 -0.111397  0.092649  0.094472  0.042676  0.071391   43549  \n",
       "1 -0.115335  0.005536  0.114284  0.120367 -0.037177   77432  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_emb.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7021d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b558e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.]],\n",
       "\n",
       "        [[ 6.,  7.,  8.],\n",
       "         [ 9., 10., 11.]],\n",
       "\n",
       "        [[12., 13., 14.],\n",
       "         [15., 16., 17.]],\n",
       "\n",
       "        [[18., 19., 20.],\n",
       "         [21., 22., 23.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.arange(24).reshape(4,2,3).float()\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c6e92fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5000,  2.5000,  3.5000],\n",
       "        [ 7.5000,  8.5000,  9.5000],\n",
       "        [13.5000, 14.5000, 15.5000],\n",
       "        [19.5000, 20.5000, 21.5000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(aa, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b12f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc2021_demo",
   "language": "python",
   "name": "conda_wbdc2021_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
